{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is part 2 of the fake news classifier and runs only the grid search with cross validation and provides the final verdict.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports as the other notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a continuation of the previous notebook but has been separated for running a Grid Search with 5 cross validations only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the first cell with all imports for throughout \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# For saving the models later. \n",
    "import joblib\n",
    "\n",
    "# Importing wordcloud and its necessary stopwords package\n",
    "#import wordcloud\n",
    "#from wordcloud import STOPWORDS\n",
    "\n",
    "# Natural Language Processing Tool Kit Imports\n",
    "# Importing Natural Language ToolKit and its essential packages\n",
    "import nltk\n",
    "\n",
    "# For seeing and removing stopwords\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "# For lemmatizing our words \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# For stemming our words \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Cleaning tools imports \n",
    "# Importing string for cleaning punctuations\n",
    "import string \n",
    "\n",
    "# Importing Regex\n",
    "import re\n",
    "\n",
    "# Vectorizing Imports\n",
    "# Importing CountVectorizer to tokenize our articles\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# For making training and testing splits prior to modelling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing Scaler for Scaling Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Importing the different models for modelling purposes\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Importing metrics to evaluate our model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# For building up a pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# For a cross-validated grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Stopping warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines & GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the model scores above, we're going to run a GridSearchCv on the models that scored over 85%, a number chosen arbitrarily. We could've chosen 80% or 90% but 80% would mean more computation power while 90% will be extremely high even though it'll require lesser models being put into our GridSearch. We will run a GridSearch for the following models:\n",
    "\n",
    "- Logistic Regression\n",
    "- Decision Tree Classifier\n",
    "- Random Forest Classifier\n",
    "- Neural Network MLP Classifier\n",
    "- Naive Bayes\n",
    "- AdaBoost Classifier\n",
    "- Support Vector Machines Classifier\n",
    "\n",
    "\n",
    "First, we're going to start off with the importing the new data frame we exported post processing. Then we're going to define the list of stopwords that we created and then finally load up all the models prior to running our grid search. \n",
    "\n",
    "**Note:** The Grid Search with cross validation along with all these models to find out the best model will take a lot of computation power and time hence, we're only going to do it for 10% of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>Title Word Count</th>\n",
       "      <th>Text Word Count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>after trump disclosures uks may says will cont...</td>\n",
       "      <td>london reuters  british prime minister theresa...</td>\n",
       "      <td>Political News</td>\n",
       "      <td>May 17, 2017</td>\n",
       "      <td>13</td>\n",
       "      <td>335</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>senators close to finishing encryption penalti...</td>\n",
       "      <td>washington reuters  technology companies could...</td>\n",
       "      <td>Political News</td>\n",
       "      <td>March 9, 2016</td>\n",
       "      <td>8</td>\n",
       "      <td>409</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>honduran opposition candidate nasralla says i ...</td>\n",
       "      <td>mexico city reuters  honduran opposition candi...</td>\n",
       "      <td>World News</td>\n",
       "      <td>December 23, 2017</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>polands pm szydlo to reshuffle cabinet soon</td>\n",
       "      <td>warsaw reuters  poland s prime minister beata ...</td>\n",
       "      <td>World News</td>\n",
       "      <td>October 24, 2017</td>\n",
       "      <td>7</td>\n",
       "      <td>397</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uber agrees to settle us lawsuit filed by indi...</td>\n",
       "      <td>san francisco reuters  uber technologies inc a...</td>\n",
       "      <td>World News</td>\n",
       "      <td>December 9, 2017</td>\n",
       "      <td>11</td>\n",
       "      <td>481</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  after trump disclosures uks may says will cont...   \n",
       "1  senators close to finishing encryption penalti...   \n",
       "2  honduran opposition candidate nasralla says i ...   \n",
       "3        polands pm szydlo to reshuffle cabinet soon   \n",
       "4  uber agrees to settle us lawsuit filed by indi...   \n",
       "\n",
       "                                                text         subject  \\\n",
       "0  london reuters  british prime minister theresa...  Political News   \n",
       "1  washington reuters  technology companies could...  Political News   \n",
       "2  mexico city reuters  honduran opposition candi...      World News   \n",
       "3  warsaw reuters  poland s prime minister beata ...      World News   \n",
       "4  san francisco reuters  uber technologies inc a...      World News   \n",
       "\n",
       "                 date  Title Word Count  Text Word Count  label  \n",
       "0       May 17, 2017                 13              335      1  \n",
       "1      March 9, 2016                  8              409      1  \n",
       "2  December 23, 2017                 20              100      1  \n",
       "3   October 24, 2017                  7              397      1  \n",
       "4   December 9, 2017                 11              481      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing file for gridsearch\n",
    "combined_df_full = pd.read_csv('combined_df.csv')\n",
    "combined_df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>Title Word Count</th>\n",
       "      <th>Text Word Count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>china criticises india over crashed drone on b...</td>\n",
       "      <td>beijing reuters  china expressed  strong dissa...</td>\n",
       "      <td>World News</td>\n",
       "      <td>December 7, 2017</td>\n",
       "      <td>8</td>\n",
       "      <td>451</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the new york times just fired back at trump w...</td>\n",
       "      <td>donald trump pissed off the new york times and...</td>\n",
       "      <td>World News</td>\n",
       "      <td>March 29, 2017</td>\n",
       "      <td>15</td>\n",
       "      <td>493</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>carrier to employees donald trump lied we’re ...</td>\n",
       "      <td>while donald trump may have bribed carrier int...</td>\n",
       "      <td>World News</td>\n",
       "      <td>December 3, 2016</td>\n",
       "      <td>14</td>\n",
       "      <td>463</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a better future  britains may tries to rally h...</td>\n",
       "      <td>london reuters  prime minister theresa may wil...</td>\n",
       "      <td>World News</td>\n",
       "      <td>September 29, 2017</td>\n",
       "      <td>11</td>\n",
       "      <td>695</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deadline nears for catalan leader to clarify i...</td>\n",
       "      <td>madrid reuters  catalan leader carles puigdemo...</td>\n",
       "      <td>World News</td>\n",
       "      <td>October 15, 2017</td>\n",
       "      <td>9</td>\n",
       "      <td>482</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  china criticises india over crashed drone on b...   \n",
       "1   the new york times just fired back at trump w...   \n",
       "2   carrier to employees donald trump lied we’re ...   \n",
       "3  a better future  britains may tries to rally h...   \n",
       "4  deadline nears for catalan leader to clarify i...   \n",
       "\n",
       "                                                text     subject  \\\n",
       "0  beijing reuters  china expressed  strong dissa...  World News   \n",
       "1  donald trump pissed off the new york times and...  World News   \n",
       "2  while donald trump may have bribed carrier int...  World News   \n",
       "3  london reuters  prime minister theresa may wil...  World News   \n",
       "4  madrid reuters  catalan leader carles puigdemo...  World News   \n",
       "\n",
       "                  date  Title Word Count  Text Word Count  label  \n",
       "0    December 7, 2017                  8              451      1  \n",
       "1       March 29, 2017                15              493      0  \n",
       "2     December 3, 2016                14              463      0  \n",
       "3  September 29, 2017                 11              695      1  \n",
       "4    October 15, 2017                  9              482      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For running the gridsearch, we will work with 10% of the data only as it is computationally very, very heavy\n",
    "combined_df = combined_df_full.sample(frac = 0.10).copy()\n",
    "combined_df.reset_index(drop = True, inplace = True)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting our data again\n",
    "X = combined_df['text']\n",
    "y = combined_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import all our models for our GridSearch\n",
    "#Saving Logistic Regression models\n",
    "LR_model = joblib.load('LR_model.pkl')\n",
    "\n",
    "#Saving Logistic Regression Scaled model\n",
    "LRSS_model = joblib.load('LRSS_model.pkl')\n",
    "\n",
    "#Saving Decision Tree model\n",
    "DT_model = joblib.load('DT_model.pkl')\n",
    "\n",
    "#Saving Random Forest model\n",
    "RF_model = joblib.load('RF_model.pkl')\n",
    "\n",
    "#Saving Neural Network model\n",
    "NN_model = joblib.load('NN_model.pkl')\n",
    "\n",
    "#Saving Naive Bayes model\n",
    "NB_model = joblib.load('NB_model.pkl')\n",
    "\n",
    "#SAving AdaBoost model\n",
    "ADB_model = joblib.load('ADB_model.pkl')\n",
    "\n",
    "# Saving Support Vector Machines models\n",
    "SVC_model = joblib.load('SVC_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Adding more words to the listofstopwords\n",
    "# Here we're going to import nltk stopwords and assigning it to a listofstopwords so we can extend the list and\n",
    "# add more words to it, words that we see above which may overlap as well as the ones pulled out from the wordcloud. \n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "listofstopwords = list(stopwords)\n",
    "listofstopwords.extend(('said','trump','reuters','president','state','government','states','new','house','united',\n",
    "                       'clinton','obama','donald','like','news','just', 'campaign', 'washington', 'election',\n",
    "                        'party', 'republican'))\n",
    "\n",
    "listofstopwords.extend(('say','obama','(reuters)','govern','news','united', 'states', '-', 'said', 'arent', 'couldnt',\n",
    "                        'didnt', 'doesnt', 'dont', 'hadnt', 'hasnt', 'havent','isnt', 'mightnt', 'mustnt', 'neednt',\n",
    "                        'shant', 'shes', 'shouldnt', 'shouldve','thatll', 'wasnt', 'werent', 'wont', 'wouldnt',\n",
    "                        'youd','youll', 'youre', 'youve', 'trump', 'democrat', 'white', 'black', 'reuter', 'monday',\n",
    "                        'tuesday','wednesday','thursday', 'friday','saturday','sunday'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "#stopwords = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def my_lemmatization_tokenizer(text):\n",
    "    \n",
    "    for word in text:\n",
    "        listofwords = text.split(' ')\n",
    "        \n",
    "    listoflemmatized_words = []\n",
    "    \n",
    "    \n",
    "    for word in listofwords:\n",
    "        if (not word in listofstopwords) and (word != ''):\n",
    "            lemmatized_word = lemmatizer.lemmatize(word)\n",
    "            listoflemmatized_words.append(lemmatized_word)\n",
    "            \n",
    "    return listoflemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below we create our pipeline where the stpes will act as placeholders and change when it gets into the GSCV\n",
    "# Let's instantiate this belo\n",
    "pipe = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                 ('model', LogisticRegression())])\n",
    "\n",
    "# Below we create a list of parametres we will be using for our GridSearchCV\n",
    "c_values = [.01, .1, 1, 10, 100, 100]\n",
    "vectorizers = [CountVectorizer(stop_words = listofstopwords, tokenizer = my_lemmatization_tokenizer),\n",
    "               TfidfVectorizer(stop_words = listofstopwords, tokenizer = my_lemmatization_tokenizer)]\n",
    "ngram_range = [(1,2), (1,3)]\n",
    "min_df = [100, 200, 300, 400, 500] # this has been reduced as we're only taking a limited sample \n",
    "max_depths = [20, 40, 60, 80]\n",
    "n_estimators = [100, 150, 200, 250]\n",
    "NB_alphas = [0, 0.5, 1.0, 10]\n",
    "NN_alphas = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "\n",
    "param_gridLR = {'vectorizer' : vectorizers,\n",
    "                'vectorizer__min_df' : min_df,\n",
    "               'vectorizer__ngram_range' : ngram_range,\n",
    "                'model' : [LogisticRegression()],\n",
    "                'model__C' : c_values}\n",
    "\n",
    "param_gridTrees = {'vectorizer' : vectorizers,\n",
    "                   'vectorizer__min_df' : min_df,\n",
    "                   'vectorizer__ngram_range' : ngram_range,\n",
    "                   'model' : [DecisionTreeClassifier(), RandomForestClassifier()],\n",
    "                   'model__max_depth' : max_depths}\n",
    "\n",
    "\n",
    "param_gridNN = {'vectorizer' : vectorizers,\n",
    "                'vectorizer__min_df' : min_df,\n",
    "                'vectorizer__ngram_range' : ngram_range,\n",
    "                'model': [MLPClassifier()],\n",
    "                'model__alpha' : NN_alphas}\n",
    "\n",
    "param_gridNB = {'vectorizer' : vectorizers,\n",
    "                'vectorizer__min_df' : min_df,\n",
    "                'vectorizer__ngram_range' : ngram_range,\n",
    "                'model': [MultinomialNB()],\n",
    "                'model__alpha' : NB_alphas}\n",
    "\n",
    "param_gridADA = {'vectorizer' : vectorizers,\n",
    "                 'vectorizer__min_df' : min_df,\n",
    "                 'vectorizer__ngram_range' : ngram_range,\n",
    "                 'model' : [AdaBoostClassifier()],\n",
    "                 'model__n_estimators' : n_estimators}\n",
    "\n",
    "param_gridSVM = {'vectorizer' : vectorizers,\n",
    "                 'vectorizer__min_df' : min_df,\n",
    "                 'vectorizer__ngram_range' : ngram_range,\n",
    "                 'model' : [LinearSVC()]}\n",
    "\n",
    "# Creating a list of parameter grid before we put them in our GridSearch\n",
    "param_grids = [param_gridLR, param_gridTrees, param_gridNN , param_gridNB, param_gridADA, param_gridSVM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 560 candidates, totalling 2800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed: 19.4min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 45.6min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed: 81.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed: 128.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1768 tasks      | elapsed: 184.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2418 tasks      | elapsed: 251.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2800 out of 2800 | elapsed: 289.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 27s, sys: 1.32 s, total: 1min 29s\n",
      "Wall time: 4h 50min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Instantiating our GridSearchCV for logistic regression\n",
    "grid = GridSearchCV(pipe, param_grid = param_grids, cv = 5, n_jobs = -1, verbose = 1)\n",
    "\n",
    "# Fitting the GridSearchCV on X_train, y_train\n",
    "grid_fitted = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters set found on development set:\n",
      "{'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=60, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'model__max_depth': 60, 'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True,\n",
      "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
      "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
      "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
      "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
      "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
      "                            'itself', ...],\n",
      "                strip_accents=None, sublinear_tf=False,\n",
      "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
      "                use_idf=True, vocabulary=None), 'vectorizer__min_df': 100, 'vectorizer__ngram_range': (1, 3)}\n",
      "\n",
      "\n",
      "Grid best score:\n",
      "0.9138869090045141\n",
      "\n",
      "\n",
      "Best model is:\n",
      "Pipeline(memory=None,\n",
      "         steps=[('vectorizer',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=1.0, max_features=None,\n",
      "                                 min_df=100, ngram_range=(1, 3), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
      "                                             'our', 'ours', 'ourselve...\n",
      "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
      "                                        class_weight=None, criterion='gini',\n",
      "                                        max_depth=60, max_features='auto',\n",
      "                                        max_leaf_nodes=None, max_samples=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=1, min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=None,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# Checking for best parameters\n",
    "print()\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(grid_fitted.best_params_)\n",
    "print()\n",
    "\n",
    "# Checking for best score\n",
    "print()\n",
    "print(\"Grid best score:\")\n",
    "print (grid_fitted.best_score_)\n",
    "print()\n",
    "\n",
    "#Checking for best model\n",
    "print()\n",
    "print(\"Best model is:\")\n",
    "print(grid_fitted.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9213197969543148"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scoring our test set on the gridsearch model.\n",
    "grid_fitted.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GridSearchCVTrained.h5']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model\n",
    "joblib.dump(grid_fitted, 'GridSearchCVTrained.pkl')\n",
    "joblib.dump(grid_fitted, 'GridSearchCVTrained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([90.45750632, 91.29571242, 90.40830789, 89.4404407 , 88.14379263,\n",
       "        89.25659184, 89.3679153 , 88.83359513, 87.81310701, 88.55097466,\n",
       "        87.72677383, 88.96520467, 88.18350916, 89.00383449, 89.2412158 ,\n",
       "        89.16892381, 88.84260058, 89.31360545, 87.86541018, 88.9003655 ,\n",
       "        88.00577431, 89.17222548, 92.36915326, 89.95662417, 88.92281857,\n",
       "        92.63250299, 88.89723063, 89.31614094, 88.17030215, 89.48371658,\n",
       "        88.27223921, 89.58349872, 90.15021462, 90.20257087, 88.26545601,\n",
       "        89.66300287, 86.07591739, 87.2058136 , 85.7158505 , 86.82307038,\n",
       "        85.57852058, 88.56091185, 85.7468667 , 87.27533422, 86.85980635,\n",
       "        87.99741273, 85.74789133, 87.14988618, 85.28563037, 86.49766688,\n",
       "        85.32767806, 86.97160063, 86.53415394, 87.12137418, 86.20897713,\n",
       "        87.54973469, 85.78815961, 87.46439109, 86.22321739, 87.49697909,\n",
       "        86.76735811, 87.66110516, 86.69479766, 88.2879015 , 87.52166638,\n",
       "        89.39716887, 88.73308377, 87.14908104, 85.91397095, 86.78229523,\n",
       "        85.60670338, 86.57204285, 86.04261465, 87.88785324, 86.26258473,\n",
       "        86.9024138 , 86.08998098, 86.82307916, 85.46429667, 86.35308428,\n",
       "        85.78640928, 86.86501279, 86.25174131, 87.62095957, 87.69559426,\n",
       "        88.98403873, 86.33597803, 86.92462349, 85.40693007, 87.7941463 ,\n",
       "        85.51112089, 86.91612091, 86.68282433, 87.30423031, 85.44442372,\n",
       "        87.9572855 , 86.44039254, 87.09802079, 85.72787418, 86.50759821,\n",
       "        85.44014955, 86.86439147, 85.88612661, 87.12017179, 85.79049392,\n",
       "        87.16453466, 85.74415202, 86.99455118, 85.90735235, 86.54774718,\n",
       "        86.13493471, 86.78332705, 87.05334039, 88.43790202, 86.0299777 ,\n",
       "        88.77352128, 85.76394629, 88.85519166, 86.79007602, 87.29839554,\n",
       "        86.12977214, 87.03820605, 85.72223382, 86.74073524, 85.50967612,\n",
       "        86.69353456, 85.93031416, 87.04187088, 85.53805099, 86.61542039,\n",
       "        85.57191105, 86.88200221, 85.82465224, 86.6916224 , 85.49210687,\n",
       "        86.81731234, 85.34114709, 86.8464704 , 85.69487276, 86.87211032,\n",
       "        85.52903461, 86.77929201, 85.41493697, 86.89112573, 85.89474349,\n",
       "        86.67406564, 85.39250116, 86.55404401, 85.35269523, 86.59521556,\n",
       "        85.51138625, 86.81495614, 85.51955304, 86.57972493, 85.29113226,\n",
       "        86.86402001, 85.5177043 , 86.60165176, 85.25501666, 86.70775657,\n",
       "        85.58579116, 86.88286304, 85.5990242 , 86.55619578, 85.55785503,\n",
       "        86.70737009, 85.63640914, 86.92336416, 85.64850154, 86.6076426 ,\n",
       "        85.53600831, 86.72186718, 85.57480941, 86.85561137, 85.87827539,\n",
       "        86.66497364, 85.40917525, 86.7971221 , 85.49411445, 86.64422932,\n",
       "        85.87629213, 86.57538414, 85.28094578, 86.92882271, 85.57281661,\n",
       "        86.92785048, 85.51531806, 86.51820402, 85.50115709, 86.78262715,\n",
       "        85.7016026 , 86.82919798, 85.49629102, 86.48875146, 85.37558208,\n",
       "        86.47526207, 85.45584221, 86.95527949, 85.54915843, 86.60671611,\n",
       "        85.55574999, 86.93689113, 85.85410976, 87.30700254, 85.78474693,\n",
       "        86.83524885, 85.54178209, 86.99812546, 85.6929266 , 86.94919086,\n",
       "        85.94674187, 86.8975121 , 85.75928144, 87.24255109, 86.13488913,\n",
       "        87.17297139, 85.58415966, 86.56568255, 85.44935293, 86.86531529,\n",
       "        85.79183941, 87.13931818, 85.85922103, 87.0186656 , 85.5222682 ,\n",
       "        87.21076875, 85.76298213, 86.40479069, 85.21760893, 86.80904369,\n",
       "        85.87929621, 87.30418482, 86.10344896, 87.55921135, 86.29278178,\n",
       "        86.99540763, 86.21279845, 86.59722767, 86.31614661, 86.79636531,\n",
       "        86.32006545, 87.81169071, 85.98070192, 87.16696472, 86.07922902,\n",
       "        87.12289457, 85.258565  , 86.94823003, 85.75873423, 86.68911371,\n",
       "        85.74739227, 87.06089811, 85.81704521, 87.21359444, 86.15935469,\n",
       "        87.41703577, 86.09282646, 87.40245728, 85.57698746, 86.71317062,\n",
       "        86.40718775, 87.93136115, 85.86376982, 87.40892792, 86.42624569,\n",
       "        86.97694216, 85.84142613, 86.82228208, 85.48905425, 86.35965142,\n",
       "        85.878546  , 87.24090953, 85.7053772 , 87.17720361, 85.93960891,\n",
       "        86.84338231, 85.69006572, 86.81115155, 85.70550337, 86.90545478,\n",
       "        86.43836884, 87.2674273 , 86.01347213, 87.51407695, 86.45192542,\n",
       "        87.50559077, 85.00252295, 86.29613004, 84.87297292, 85.95719895,\n",
       "        86.45584774, 87.38115978, 86.05257096, 87.22312155, 85.7716682 ,\n",
       "        86.57205062, 85.46212668, 86.11305432, 84.96247778, 85.75721993,\n",
       "        86.74846063, 87.65189309, 86.28595676, 87.24450712, 86.26068335,\n",
       "        87.20573387, 85.28624249, 86.1296299 , 85.24120178, 85.70191979,\n",
       "        86.68312583, 87.30524602, 86.18683438, 87.66071897, 85.26712852,\n",
       "        86.63541265, 85.21604161, 86.35864129, 85.65216684, 86.07794914,\n",
       "        87.1452261 , 87.70233197, 86.88617191, 88.11479397, 86.82050643,\n",
       "        88.16931391, 85.29110365, 86.63788815, 85.45081258, 86.33163791,\n",
       "        86.73185492, 88.04113326, 86.00628924, 87.00027537, 85.83870955,\n",
       "        87.39347849, 85.94650469, 86.96879416, 85.16932616, 86.13686237,\n",
       "        86.57293057, 87.83485351, 86.46824136, 87.60227451, 86.090134  ,\n",
       "        87.3536973 , 85.2365345 , 86.83658853, 84.99873819, 85.89493704,\n",
       "        86.28394651, 86.77002797, 85.34875326, 87.55500975, 85.97682967,\n",
       "        86.91899495, 85.45142884, 86.45971498, 85.3993434 , 86.09637008,\n",
       "        86.15513272, 86.98662167, 85.60937443, 87.77938657, 86.05947895,\n",
       "        87.09987755, 85.75985317, 86.16491804, 85.08294654, 86.66520572,\n",
       "        86.2300303 , 87.15326262, 86.18637962, 86.9705976 , 85.8626266 ,\n",
       "        87.00826283, 85.62195921, 86.54811072, 85.19025168, 86.01624494,\n",
       "        84.74253888, 85.87160511, 85.56487255, 86.37401471, 85.48786798,\n",
       "        86.27600074, 85.51074448, 86.04817448, 85.16224208, 86.01857748,\n",
       "        84.48837972, 85.98140473, 84.88600321, 85.85603929, 84.74505939,\n",
       "        86.12350101, 84.8848074 , 85.53352542, 84.98065338, 85.68453608,\n",
       "        84.77258644, 86.12430916, 84.96174846, 85.82329569, 84.49985485,\n",
       "        85.67418661, 84.84178586, 86.21362948, 85.57086854, 85.6049448 ,\n",
       "        85.33396344, 86.47978148, 84.96732039, 85.4398469 , 84.84757361,\n",
       "        85.62919745, 84.73984375, 85.89915309, 84.83946528, 85.70166078,\n",
       "        84.45494294, 85.6839386 , 84.70880976, 85.76181188, 85.05447955,\n",
       "        85.7905313 , 84.41840692, 85.86731734, 84.71093111, 85.64548783,\n",
       "        84.89334159, 85.93010769, 84.48857832, 85.9063343 , 85.01847959,\n",
       "        85.86502628, 84.73017969, 85.80613289, 84.72250266, 85.66990337,\n",
       "        84.81854239, 85.70660453, 84.88875651, 85.81494141, 84.55808916,\n",
       "        85.70845151, 84.91110172, 85.96111798, 84.73305554, 85.64892197,\n",
       "        84.80866294, 85.58198433, 84.69794893, 85.92290044, 84.93618507,\n",
       "        85.39205575, 84.57887115, 85.70008984, 84.7390645 , 85.86057472,\n",
       "        85.06534538, 85.89297414, 84.78777599, 86.02354074, 85.48275566,\n",
       "        86.25658822, 84.83838191, 85.75769939, 84.93734159, 85.78222566,\n",
       "        85.35184989, 86.20022445, 85.07786274, 86.04322767, 85.07219315,\n",
       "        86.04889593, 84.94472075, 86.18656869, 84.79208555, 85.86701741,\n",
       "        85.04442906, 86.29052896, 85.3879662 , 86.22968888, 85.21776185,\n",
       "        86.17970519, 84.82500596, 86.04734726, 84.97938056, 85.91605797,\n",
       "        85.3392323 , 86.43497896, 85.0532268 , 86.44593773, 85.33082247,\n",
       "        86.42500849, 84.54151397, 85.65916224, 84.77510095, 85.85106344,\n",
       "        86.56327887, 86.3921083 , 85.31150031, 86.73380756, 85.95882082,\n",
       "        86.31778102, 85.12654099, 85.92008896, 84.72230878, 85.69327297,\n",
       "        85.63913193, 86.73528752, 85.45106554, 86.31625381, 85.2648726 ,\n",
       "        86.18261347, 84.66414127, 86.03345623, 84.99887934, 85.73988924,\n",
       "        85.26971006, 86.49191451, 85.30875912, 86.75107903, 85.45441189,\n",
       "        86.34739532, 84.61170502, 85.4949883 , 84.87114253, 86.22815757,\n",
       "        85.99272447, 86.99603524, 85.33267899, 86.45695024, 85.63882904,\n",
       "        86.31244383, 84.89241848, 85.88009987, 84.77074175, 85.82554712,\n",
       "        84.96113968, 85.68632674, 84.84441147, 85.88103862, 84.82444334,\n",
       "        85.57694464, 85.07477074, 86.06122603, 86.10565057, 85.90854497,\n",
       "        84.7044066 , 86.45905657, 85.79128561, 85.9331996 , 85.29395108,\n",
       "        87.85374026, 84.77001004, 84.48741446, 70.58730197, 68.28985062]),\n",
       " 'std_fit_time': array([1.91831532, 1.44420044, 1.31170924, 2.00808743, 1.72566825,\n",
       "        1.43504716, 1.30574159, 1.35453465, 1.5912663 , 1.34699394,\n",
       "        1.69239578, 1.4682767 , 1.58765952, 1.44770939, 2.17784687,\n",
       "        1.44492886, 1.37024847, 1.13995933, 1.27567645, 1.4427585 ,\n",
       "        1.42292554, 1.21392989, 5.3670976 , 1.49136254, 1.85214242,\n",
       "        6.04161674, 1.37852086, 1.39695325, 1.00183949, 1.09291172,\n",
       "        1.29099821, 1.63728911, 1.97563823, 2.05126353, 1.73485926,\n",
       "        2.9651845 , 1.97573933, 1.41529044, 1.31848945, 1.36809506,\n",
       "        1.48990756, 1.93117445, 1.39033782, 2.44262398, 1.5359055 ,\n",
       "        1.73454831, 1.34086216, 1.42490504, 1.2438698 , 1.43826655,\n",
       "        1.67720679, 1.38740449, 1.43353532, 0.98579639, 1.91576077,\n",
       "        0.99388506, 1.36318193, 1.55339272, 1.73786904, 1.77081282,\n",
       "        1.24092333, 1.22428638, 2.66180798, 0.89911212, 1.6484544 ,\n",
       "        1.99752604, 1.77647388, 1.2585515 , 1.4085597 , 1.30596598,\n",
       "        1.36684947, 1.50502496, 1.52950766, 2.0034078 , 2.27978581,\n",
       "        1.16690518, 1.15202859, 1.12329561, 1.14020249, 1.33615751,\n",
       "        1.00675734, 1.53451972, 1.43525306, 2.44767288, 5.08831928,\n",
       "        3.79379153, 1.16833873, 1.66531022, 1.32216126, 2.80340135,\n",
       "        1.48434104, 1.3612081 , 1.14040244, 1.17921383, 1.53638023,\n",
       "        1.71630076, 1.20938489, 1.08604345, 1.57695452, 1.16900964,\n",
       "        1.24621419, 1.34475259, 1.63587501, 1.46983746, 1.49234688,\n",
       "        1.61317235, 1.14469474, 1.31175318, 1.5425716 , 1.44471174,\n",
       "        1.79984536, 1.45490947, 2.50224535, 2.23495158, 1.54191068,\n",
       "        1.5817276 , 1.29780937, 2.20136729, 2.08341916, 2.37078504,\n",
       "        2.22048891, 1.43014677, 1.21378779, 1.24492953, 1.32258628,\n",
       "        1.38565783, 1.36644014, 1.60995942, 1.43736417, 1.13747353,\n",
       "        1.11239074, 1.57544168, 1.26841042, 1.47890886, 1.29190411,\n",
       "        1.37191916, 1.40825883, 1.444867  , 1.36189739, 1.16530067,\n",
       "        1.52663786, 1.08481949, 1.2664691 , 1.40190287, 1.48391448,\n",
       "        1.42866459, 1.52519973, 1.38921998, 1.29801799, 1.36019061,\n",
       "        1.51913181, 1.43864497, 1.4635956 , 1.24745859, 1.64830517,\n",
       "        1.48809584, 1.49396868, 1.25741704, 1.42012179, 1.0978486 ,\n",
       "        1.37731198, 1.05516199, 1.54118149, 1.41998166, 1.40735433,\n",
       "        1.14123066, 1.34446343, 1.39659652, 1.48893322, 1.30706761,\n",
       "        1.5014205 , 1.18598447, 1.43682295, 1.26401812, 1.33874745,\n",
       "        1.12146228, 1.19219138, 1.24274173, 1.25910588, 1.30274612,\n",
       "        1.49675795, 1.5547757 , 1.28323224, 1.36723267, 1.14301865,\n",
       "        1.297885  , 1.40409055, 1.35053463, 1.21173404, 1.41281512,\n",
       "        1.42031923, 1.2171518 , 1.23372966, 1.36949064, 1.51563788,\n",
       "        1.51607329, 1.31548943, 1.3438107 , 1.32850404, 1.25164471,\n",
       "        1.28998618, 1.18862953, 1.37618554, 1.59106776, 1.34582523,\n",
       "        1.61649868, 1.35628445, 1.34051765, 1.31900779, 1.74628743,\n",
       "        1.24549707, 1.2926629 , 1.48300571, 1.30388545, 1.53366912,\n",
       "        1.5524277 , 1.49493413, 1.34154338, 1.53109695, 1.41062537,\n",
       "        1.32109366, 1.4493692 , 1.60458935, 1.48395791, 1.39465701,\n",
       "        1.54010784, 1.27607884, 1.10375856, 1.38752987, 1.29724842,\n",
       "        1.22065363, 1.36955643, 1.52575241, 1.09445216, 1.31405782,\n",
       "        1.42287198, 2.21654113, 1.33802026, 1.50147115, 1.43371671,\n",
       "        2.4384426 , 2.60463239, 1.3460547 , 1.7714969 , 1.5217272 ,\n",
       "        1.41744965, 1.12549454, 1.18707382, 1.4020169 , 1.40389765,\n",
       "        1.35840187, 1.54706063, 1.5713317 , 1.44360806, 1.21583523,\n",
       "        1.30009064, 2.23340327, 2.37199739, 1.53597122, 1.56054553,\n",
       "        1.67737975, 2.23306392, 1.49160425, 1.17264227, 2.33874497,\n",
       "        1.51708353, 1.54673141, 1.13198922, 1.46414369, 1.48676544,\n",
       "        1.2516035 , 1.40939509, 1.43622783, 1.44385741, 1.44419263,\n",
       "        1.4939077 , 1.52845227, 1.59256904, 1.39498534, 1.69468085,\n",
       "        1.38603069, 1.45057683, 1.45537555, 1.68303252, 1.5512859 ,\n",
       "        1.44228307, 1.03694069, 1.56840762, 1.58175501, 1.49525153,\n",
       "        1.44008574, 1.43795904, 1.21427727, 1.65948883, 1.37258598,\n",
       "        1.6350221 , 1.43960941, 1.67986071, 1.56916392, 1.65108562,\n",
       "        1.49285891, 1.50333802, 1.44226455, 1.56365758, 1.61851335,\n",
       "        1.50403127, 1.55311363, 0.84530581, 1.17663334, 1.33255721,\n",
       "        1.59160408, 1.36521908, 1.36679198, 0.8983007 , 1.33691953,\n",
       "        1.72830196, 1.74991112, 1.18702512, 1.40296002, 0.98179825,\n",
       "        1.56542924, 1.41264962, 2.02688877, 1.09898474, 1.84882515,\n",
       "        1.5670217 , 1.41588258, 1.23291511, 1.25231141, 1.71715224,\n",
       "        1.56039852, 1.32369687, 1.50678609, 0.92721857, 1.34925802,\n",
       "        1.48541351, 1.63991015, 1.27913737, 1.66700104, 1.44775634,\n",
       "        1.46786557, 1.70497547, 2.02174725, 1.50348969, 1.54549054,\n",
       "        2.00473987, 1.66341902, 1.66147454, 1.61363941, 1.36696597,\n",
       "        1.64018712, 1.24899724, 1.27233178, 1.57855305, 1.65722713,\n",
       "        1.59121807, 1.51136393, 1.3306429 , 1.38024984, 1.63218259,\n",
       "        1.55080316, 1.72431003, 1.62039276, 1.41314215, 1.45791086,\n",
       "        1.51277339, 1.46142699, 1.52670719, 1.24632243, 1.25719372,\n",
       "        1.02910572, 1.98739174, 1.50553725, 1.26376963, 1.23484866,\n",
       "        1.69922718, 1.28343603, 1.73915044, 1.90932894, 1.34013844,\n",
       "        1.43467339, 1.62854512, 1.3597129 , 1.39397632, 1.87927619,\n",
       "        1.30529655, 1.50109887, 1.24477595, 1.40981463, 1.49955104,\n",
       "        1.44621732, 1.2717794 , 1.58342535, 1.42187726, 1.64261614,\n",
       "        1.2340794 , 1.60997598, 1.19115675, 1.51809883, 1.43864413,\n",
       "        1.57441545, 1.35021048, 1.50829479, 1.3672249 , 1.53973337,\n",
       "        1.34216162, 1.47926472, 1.05371542, 1.70143036, 1.16862278,\n",
       "        2.32133672, 2.2247195 , 1.43336125, 1.57911109, 1.57485071,\n",
       "        1.54030462, 1.51312549, 1.38809148, 1.08761782, 1.1958539 ,\n",
       "        1.3599769 , 1.44848736, 1.48498436, 1.59325773, 1.16588463,\n",
       "        1.50996699, 1.34920694, 1.54563541, 1.71183367, 1.34971633,\n",
       "        1.30038696, 1.09729869, 1.53576664, 1.53458551, 1.24822932,\n",
       "        1.18216972, 1.28353368, 1.19152502, 1.51643425, 1.49247659,\n",
       "        1.47506337, 1.3705705 , 1.54037393, 1.57629373, 1.51002863,\n",
       "        1.45443441, 1.41125491, 1.08788384, 1.47349472, 1.36580442,\n",
       "        1.53755977, 1.79390818, 1.37471548, 1.41960919, 1.16183236,\n",
       "        1.39080224, 1.36866819, 1.62067039, 1.23395007, 1.45823905,\n",
       "        1.44670312, 1.3688258 , 1.42322147, 1.08495171, 1.43705202,\n",
       "        1.37471486, 1.69454885, 1.3771165 , 1.28887814, 1.36611042,\n",
       "        1.47557397, 1.35024414, 1.43277284, 1.31216881, 1.42475877,\n",
       "        1.45962026, 1.37220223, 1.26760924, 1.7072887 , 1.50574278,\n",
       "        1.63457405, 1.60835596, 1.27860492, 1.38443662, 1.41033006,\n",
       "        1.18727709, 1.49746362, 1.43457247, 1.2496653 , 1.60242081,\n",
       "        1.2149162 , 1.30774534, 1.56643992, 1.08458525, 1.33800551,\n",
       "        1.13631521, 1.29950498, 1.38655007, 1.48338593, 1.54721985,\n",
       "        1.70419632, 1.46672354, 1.19641518, 2.35821471, 2.08781427,\n",
       "        1.158585  , 1.34257848, 1.66614546, 1.40695632, 1.2267059 ,\n",
       "        1.35561046, 1.41427201, 1.51668626, 1.15583071, 1.23158577,\n",
       "        1.37159452, 1.50824494, 1.74071456, 1.47946303, 1.3894604 ,\n",
       "        1.2628922 , 1.34479209, 1.22947863, 1.26996912, 1.44404971,\n",
       "        1.28218806, 1.39919689, 1.44115174, 1.32028366, 1.27541567,\n",
       "        1.65243393, 1.11207407, 1.35928702, 1.39105888, 1.17682135,\n",
       "        1.58934922, 1.32070177, 1.49405987, 1.50847963, 1.32263875,\n",
       "        1.2671434 , 1.23366758, 1.49797097, 1.47950882, 1.48765538,\n",
       "        1.35898029, 1.35441633, 1.31251796, 1.8166315 , 1.40759182,\n",
       "        1.36720583, 2.41458169, 1.72540619, 1.32712941, 1.39753556,\n",
       "        2.3585453 , 1.42975205, 3.27627041, 2.79935217, 7.8940577 ]),\n",
       " 'mean_score_time': array([22.0432395 , 22.15725551, 22.49255524, 22.70084167, 23.27349267,\n",
       "        22.78500552,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        22.80847602, 22.37382364, 22.75498972, 23.31055107, 22.70353937,\n",
       "        23.13505721,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        23.41918068, 23.19327769, 23.08617001, 22.82811317, 22.60451336,\n",
       "        22.15511699,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        23.9494863 , 22.97475328, 22.57403445, 21.94168921, 21.4930851 ,\n",
       "        21.69686117,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.40471277, 22.40044141, 21.63163939, 21.30986466, 21.81829352,\n",
       "        21.49399705,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.90875955, 21.43527141, 21.71293149, 21.3928854 , 21.54425025,\n",
       "        21.65352044,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.75392866, 22.40916867, 22.05796118, 22.0660953 , 22.01840739,\n",
       "        22.01719456,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.51590505, 21.81754708, 21.59602833, 21.65156627, 21.54940138,\n",
       "        21.41732397,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.83504586, 21.81147981, 22.42513089, 21.50706329, 21.54029284,\n",
       "        21.73336711,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.63020349, 21.81705589, 21.69956336, 21.695263  , 21.52176437,\n",
       "        21.49085417,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.70294833, 21.8079361 , 21.62437744, 21.38055091, 21.64571676,\n",
       "        21.66235776,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        22.04835076, 21.80353861, 21.85500174, 21.89453382, 21.74147687,\n",
       "        22.18705726,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.11989117, 21.1757761 , 21.08208661, 21.11550231, 21.05320792,\n",
       "        21.27064605,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.21067085, 21.20439134, 21.17396073, 21.09450903, 20.95025597,\n",
       "        21.11828604,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.12244596, 21.16511884, 21.11027632, 21.18111973, 21.0183075 ,\n",
       "        21.12789073,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.05154428, 21.20064926, 21.09727736, 21.17309952, 21.31153884,\n",
       "        21.15941315,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.19605403, 21.21884718, 21.09516358, 21.04758472, 21.06937532,\n",
       "        21.1147572 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.04998407, 21.16771636, 21.10231509, 21.07905469, 21.08345685,\n",
       "        21.07975483,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.21440845, 21.14257503, 21.0744977 , 21.14710197, 21.11048918,\n",
       "        21.10955186,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.16851773, 21.14856539, 21.10627089, 21.14300756, 21.15750403,\n",
       "        21.33532577,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.55380511, 21.67349529, 21.31725011, 21.51556735, 21.38519773,\n",
       "        21.41646514,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.5332984 , 21.37554426, 21.34199271, 21.36653371, 21.57667799,\n",
       "        21.60630984,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.59357471, 21.60528193, 21.5140646 , 21.46440682, 21.40718513,\n",
       "        21.49693971,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.60193291, 21.73744731, 21.40579195, 21.52822347, 21.6461987 ,\n",
       "        21.64759135,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.50062475, 21.76473351, 21.8112288 , 21.51715736, 21.36234794,\n",
       "        21.3991982 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.4865726 , 21.54683399, 21.39206586, 21.51081438, 21.6412415 ,\n",
       "        21.59418459,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.56583781, 21.67873282, 21.38302474, 21.54442015, 21.55043025,\n",
       "        21.68244696,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.70730014, 21.7100564 , 21.33070674, 21.34919949, 21.59099259,\n",
       "        21.54991121,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.07499728, 20.9811224 , 21.06834593, 21.1246016 , 21.08262401,\n",
       "        21.1756732 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.01325192, 21.061726  , 21.06041775, 21.15640659, 21.10776453,\n",
       "        21.18662548,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.02322655, 21.11725826, 20.97363195, 21.19349475, 21.05540152,\n",
       "        21.30573883,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        20.86528668, 21.34900908, 21.15386758, 21.08046169, 20.94503937,\n",
       "        21.17103958,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.25486155, 21.00460696, 21.29852715, 21.04992719, 21.02248979,\n",
       "        21.25877318,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        20.93758874, 21.20307903, 21.02227483, 21.03602772, 21.34961939,\n",
       "        21.09325099,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.14123526, 21.12217512, 21.02602358, 20.98422728, 21.13814855,\n",
       "        21.08673587,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.06341348, 21.1030838 , 21.27453504, 21.06178923, 21.1950798 ,\n",
       "        21.11519837,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.05124393, 21.13465676, 21.04419112, 21.17543764, 21.07221766,\n",
       "        21.06439857,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.05252194, 21.13727088, 21.09963408, 21.05372939, 21.17330055,\n",
       "        21.28551126,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.03036652, 21.30483384, 21.06239033, 21.07835665, 21.09476652,\n",
       "        21.04965305,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        20.94645319, 21.135571  , 21.01003485, 21.06377025, 20.99174037,\n",
       "        21.04002633,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        20.97185454, 21.10719242, 21.00716305, 21.09300466, 21.46102438,\n",
       "        21.02363815,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.32416148, 21.1535583 , 21.0397718 , 21.00263758, 21.00421963,\n",
       "        21.0865829 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.04929118, 21.1158731 , 21.02284822, 20.96556044, 20.93865342,\n",
       "        21.02534766,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.04649024, 21.06160936, 20.96499968, 21.1561523 , 21.02685022,\n",
       "        21.06421452,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.05789909, 21.07086229, 21.08553247, 21.07422929, 20.96201162,\n",
       "        21.03909864,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.03636131, 21.1204391 , 21.04498529, 21.04410295, 21.01805401,\n",
       "        21.04671941,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        20.99693465, 21.05177789, 20.96371355, 21.08813648, 21.02831254,\n",
       "        21.07533269,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.0194962 , 21.05738311, 21.01758556, 21.04964809, 21.02787185,\n",
       "        21.27390347,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.06245103, 21.02860193, 21.03760867, 21.00644541, 21.02801242,\n",
       "        20.92846704,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        20.96725802, 21.10084648, 21.06805701, 21.05133014, 20.9011982 ,\n",
       "        21.02817078,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.4285161 , 21.1336092 , 21.07315774, 21.17755237, 21.2354609 ,\n",
       "        21.09643941,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.08041592, 21.00395164, 21.10572114, 21.018361  , 20.95603881,\n",
       "        21.09439945,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.03644395, 21.10732708, 21.1317925 , 21.09487195, 21.13366952,\n",
       "        21.136584  ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        20.99959044, 21.15488267, 21.08079925, 21.17627211, 21.00757031,\n",
       "        21.07632713,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.03466492, 20.99036813, 20.99362144, 21.06998215, 20.97245622,\n",
       "        21.37415261,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        21.01802344, 21.50263076, 21.39275117, 20.99844341, 21.169906  ,\n",
       "        21.55284848,  0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " 'std_score_time': array([1.39855869, 1.753448  , 1.83340259, 1.38336385, 1.79758917,\n",
       "        1.68208599, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.7544129 , 2.09425219, 0.78906753, 1.96739037, 1.76035219,\n",
       "        2.41099883, 0.        , 0.        , 0.        , 0.        ,\n",
       "        2.78885909, 1.13581185, 1.70642038, 1.54037139, 1.6575932 ,\n",
       "        1.18488704, 0.        , 0.        , 0.        , 0.        ,\n",
       "        2.30114943, 1.10320953, 2.35258519, 1.37012093, 1.54086556,\n",
       "        1.519885  , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.47614459, 1.55493105, 1.4832731 , 1.34143019, 1.36053549,\n",
       "        1.43481939, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.30849039, 1.58438884, 1.30156193, 1.29729054, 1.12565038,\n",
       "        1.3322715 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.20758262, 1.52891134, 1.34964033, 1.78093584, 1.89225686,\n",
       "        1.55304332, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.08594328, 1.49340871, 1.10090012, 1.6333231 , 1.44254908,\n",
       "        1.47387912, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.49143769, 1.09471859, 2.27159327, 1.3890978 , 1.41293503,\n",
       "        1.30657887, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.25800645, 1.20350196, 1.65085666, 1.72841012, 1.52213723,\n",
       "        1.46000345, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.06694043, 1.32884643, 1.49305437, 1.19946439, 1.21929429,\n",
       "        1.36815962, 0.        , 0.        , 0.        , 0.        ,\n",
       "        2.26158322, 1.48362667, 1.53316943, 1.61221451, 1.55140033,\n",
       "        1.3333348 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.32844612, 1.44574723, 1.36093253, 1.33139935, 1.35173756,\n",
       "        1.42182085, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.45386047, 1.50149494, 1.4148949 , 1.36655547, 1.40447911,\n",
       "        1.38316483, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.38716759, 1.415835  , 1.38174079, 1.33801991, 1.36694747,\n",
       "        1.34009742, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.35324149, 1.36898765, 1.29880536, 1.41010431, 1.46498754,\n",
       "        1.33807909, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.48178937, 1.44149302, 1.38298796, 1.37616444, 1.36309125,\n",
       "        1.35623281, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.34654581, 1.40449604, 1.33706229, 1.32799547, 1.3544944 ,\n",
       "        1.32382795, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.41432219, 1.32818149, 1.33066291, 1.36493463, 1.38390618,\n",
       "        1.33863748, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.35762944, 1.40029271, 1.33966348, 1.35485324, 1.35507927,\n",
       "        1.46757653, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.58733753, 1.50631079, 1.43510514, 1.45044575, 1.36165127,\n",
       "        1.37761276, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.4425431 , 1.38886662, 1.39991895, 1.33768287, 1.41239548,\n",
       "        1.34365248, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.38057864, 1.31642131, 1.37518693, 1.37553603, 1.4318353 ,\n",
       "        1.38034392, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.55900265, 1.53735858, 1.38582933, 1.52294645, 1.75663515,\n",
       "        1.4425459 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.33370023, 1.1163069 , 1.5774954 , 1.37182266, 1.37836687,\n",
       "        1.38321496, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.38954716, 1.36874991, 1.34814924, 1.57362826, 1.61213531,\n",
       "        1.38887144, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.38253454, 1.25927112, 1.49547274, 1.53107485, 1.39380409,\n",
       "        1.50673922, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.29750823, 1.44902939, 1.40243256, 1.4181952 , 1.40946061,\n",
       "        1.33001118, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.28654817, 1.49654767, 1.32658902, 1.41170538, 1.49757325,\n",
       "        1.37571543, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.37870436, 1.42247759, 1.45201181, 1.36647275, 1.44430306,\n",
       "        1.34331157, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.45541099, 1.48661826, 1.50659464, 1.42006374, 1.4660996 ,\n",
       "        1.26326888, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.5199908 , 1.31276041, 1.29815074, 1.33085954, 1.35906688,\n",
       "        1.46664923, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.40617596, 1.38244029, 1.32044209, 1.34406839, 1.48984379,\n",
       "        1.32553291, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.37545228, 1.41635984, 1.37238772, 1.38629388, 1.33582721,\n",
       "        1.33392235, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.37341387, 1.47918828, 1.32438004, 1.31933826, 1.40476041,\n",
       "        1.36401318, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.40175625, 1.29073868, 1.33361729, 1.36992518, 1.41981861,\n",
       "        1.39492157, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.42884422, 1.32112725, 1.33585613, 1.45599591, 1.46556656,\n",
       "        1.49420827, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.39793099, 1.37977494, 1.37935218, 1.28607632, 1.47097353,\n",
       "        1.32245303, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.36357818, 1.63797294, 1.36023618, 1.36496427, 1.16747787,\n",
       "        1.33033834, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.31995323, 1.37827702, 1.29775944, 1.35981044, 1.39381558,\n",
       "        1.36634522, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.37065542, 1.31376141, 1.34817641, 1.24672882, 1.2044175 ,\n",
       "        1.46613571, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.36798166, 1.34404608, 1.33941129, 1.39650506, 1.29051937,\n",
       "        1.3807912 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.398922  , 1.3712338 , 1.42259654, 1.42943707, 1.42635488,\n",
       "        1.37771744, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.38691449, 1.41263166, 1.34053332, 1.35601151, 1.30988804,\n",
       "        1.39241683, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.47618544, 1.45624146, 1.36231816, 1.29854046, 1.2783555 ,\n",
       "        1.31498771, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.31685972, 1.36690819, 1.38701584, 1.38529844, 1.44194988,\n",
       "        1.42341144, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.40073767, 1.39823176, 1.33377262, 1.41035888, 1.39055606,\n",
       "        1.3686775 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.39787067, 1.31852291, 1.35673783, 1.47850689, 1.46598962,\n",
       "        1.39925632, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.32413633, 1.33943993, 1.38956505, 1.41184496, 1.33738128,\n",
       "        1.30291269, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.49365318, 1.29426999, 1.32222784, 1.4211245 , 1.25609064,\n",
       "        1.35004415, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.52329518, 1.39398142, 1.35110483, 1.4126662 , 1.37184056,\n",
       "        1.34665421, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.32642074, 1.43950408, 1.35991082, 1.41524787, 1.35982481,\n",
       "        1.27874998, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.46255261, 1.39613349, 1.3940027 , 1.3854998 , 1.32168289,\n",
       "        1.4240174 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.5055754 , 1.4077338 , 1.37802146, 1.46171431, 1.36407657,\n",
       "        1.3879605 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.37302655, 1.4926632 , 1.31518694, 1.3287617 , 1.26781653,\n",
       "        1.34526983, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.31533509, 1.23175357, 1.63700488, 1.3943669 , 1.62320485,\n",
       "        1.17637406, 0.        , 0.        , 0.        , 0.        ]),\n",
       " 'param_model': masked_array(data=[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=60, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       "                    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       "                    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       "                    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       "                    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       "                    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       "                    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       "                    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       "                    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       "                    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       "                    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       "                    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       "                    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       "                    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       "                    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       "                    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       "                    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       "                    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       "                    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       "                    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       "                    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "           verbose=0)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_model__C': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vectorizer': masked_array(data=[CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...],\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                 use_idf=True, vocabulary=None)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vectorizer__min_df': masked_array(data=[100, 100, 200, 200, 300, 300, 400, 400, 500, 500, 100,\n",
       "                    100, 200, 200, 300, 300, 400, 400, 500, 500, 100, 100,\n",
       "                    200, 200, 300, 300, 400, 400, 500, 500, 100, 100, 200,\n",
       "                    200, 300, 300, 400, 400, 500, 500, 100, 100, 200, 200,\n",
       "                    300, 300, 400, 400, 500, 500, 100, 100, 200, 200, 300,\n",
       "                    300, 400, 400, 500, 500, 100, 100, 200, 200, 300, 300,\n",
       "                    400, 400, 500, 500, 100, 100, 200, 200, 300, 300, 400,\n",
       "                    400, 500, 500, 100, 100, 200, 200, 300, 300, 400, 400,\n",
       "                    500, 500, 100, 100, 200, 200, 300, 300, 400, 400, 500,\n",
       "                    500, 100, 100, 200, 200, 300, 300, 400, 400, 500, 500,\n",
       "                    100, 100, 200, 200, 300, 300, 400, 400, 500, 500, 100,\n",
       "                    100, 200, 200, 300, 300, 400, 400, 500, 500, 100, 100,\n",
       "                    200, 200, 300, 300, 400, 400, 500, 500, 100, 100, 200,\n",
       "                    200, 300, 300, 400, 400, 500, 500, 100, 100, 200, 200,\n",
       "                    300, 300, 400, 400, 500, 500, 100, 100, 200, 200, 300,\n",
       "                    300, 400, 400, 500, 500, 100, 100, 200, 200, 300, 300,\n",
       "                    400, 400, 500, 500, 100, 100, 200, 200, 300, 300, 400,\n",
       "                    400, 500, 500, 100, 100, 200, 200, 300, 300, 400, 400,\n",
       "                    500, 500, 100, 100, 200, 200, 300, 300, 400, 400, 500,\n",
       "                    500, 100, 100, 200, 200, 300, 300, 400, 400, 500, 500,\n",
       "                    100, 100, 200, 200, 300, 300, 400, 400, 500, 500, 100,\n",
       "                    100, 200, 200, 300, 300, 400, 400, 500, 500, 100, 100,\n",
       "                    200, 200, 300, 300, 400, 400, 500, 500, 100, 100, 200,\n",
       "                    200, 300, 300, 400, 400, 500, 500, 100, 100, 200, 200,\n",
       "                    300, 300, 400, 400, 500, 500, 100, 100, 200, 200, 300,\n",
       "                    300, 400, 400, 500, 500, 100, 100, 200, 200, 300, 300,\n",
       "                    400, 400, 500, 500, 100, 100, 200, 200, 300, 300, 400,\n",
       "                    400, 500, 500, 100, 100, 200, 200, 300, 300, 400, 400,\n",
       "                    500, 500, 100, 100, 200, 200, 300, 300, 400, 400, 500,\n",
       "                    500, 100, 100, 200, 200, 300, 300, 400, 400, 500, 500,\n",
       "                    100, 100, 200, 200, 300, 300, 400, 400, 500, 500, 100,\n",
       "                    100, 200, 200, 300, 300, 400, 400, 500, 500, 100, 100,\n",
       "                    200, 200, 300, 300, 400, 400, 500, 500, 100, 100, 200,\n",
       "                    200, 300, 300, 400, 400, 500, 500, 100, 100, 200, 200,\n",
       "                    300, 300, 400, 400, 500, 500, 100, 100, 200, 200, 300,\n",
       "                    300, 400, 400, 500, 500, 100, 100, 200, 200, 300, 300,\n",
       "                    400, 400, 500, 500, 100, 100, 200, 200, 300, 300, 400,\n",
       "                    400, 500, 500, 100, 100, 200, 200, 300, 300, 400, 400,\n",
       "                    500, 500, 100, 100, 200, 200, 300, 300, 400, 400, 500,\n",
       "                    500, 100, 100, 200, 200, 300, 300, 400, 400, 500, 500,\n",
       "                    100, 100, 200, 200, 300, 300, 400, 400, 500, 500, 100,\n",
       "                    100, 200, 200, 300, 300, 400, 400, 500, 500, 100, 100,\n",
       "                    200, 200, 300, 300, 400, 400, 500, 500, 100, 100, 200,\n",
       "                    200, 300, 300, 400, 400, 500, 500, 100, 100, 200, 200,\n",
       "                    300, 300, 400, 400, 500, 500, 100, 100, 200, 200, 300,\n",
       "                    300, 400, 400, 500, 500, 100, 100, 200, 200, 300, 300,\n",
       "                    400, 400, 500, 500, 100, 100, 200, 200, 300, 300, 400,\n",
       "                    400, 500, 500, 100, 100, 200, 200, 300, 300, 400, 400,\n",
       "                    500, 500, 100, 100, 200, 200, 300, 300, 400, 400, 500,\n",
       "                    500, 100, 100, 200, 200, 300, 300, 400, 400, 500, 500,\n",
       "                    100, 100, 200, 200, 300, 300, 400, 400, 500, 500],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vectorizer__ngram_range': masked_array(data=[(1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2),\n",
       "                    (1, 3), (1, 2), (1, 3), (1, 2), (1, 3), (1, 2), (1, 3)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_model__max_depth': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, 20, 20, 20, 20, 20, 20,\n",
       "                    20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "                    40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,\n",
       "                    40, 40, 40, 40, 40, 40, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "                    60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 80, 80,\n",
       "                    80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "                    80, 80, 80, 80, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "                    20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 40, 40, 40, 40,\n",
       "                    40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,\n",
       "                    40, 40, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "                    60, 60, 60, 60, 60, 60, 60, 60, 80, 80, 80, 80, 80, 80,\n",
       "                    80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --],\n",
       "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_model__alpha': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --],\n",
       "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_model__n_estimators': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 150, 150, 150,\n",
       "                    150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150,\n",
       "                    150, 150, 150, 150, 150, 150, 200, 200, 200, 200, 200,\n",
       "                    200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
       "                    200, 200, 200, 200, 250, 250, 250, 250, 250, 250, 250,\n",
       "                    250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250,\n",
       "                    250, 250, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --],\n",
       "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.01,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.01,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.01,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.01,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.01,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.01,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.01,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.01,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.01,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.01,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.01,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.01,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.01,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.01,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.01,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.01,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.01,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.01,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.01,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.01,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 0.1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False),\n",
       "   'model__C': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                          random_state=None, splitter='best'),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 20,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 40,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 60,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=60, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False),\n",
       "   'model__max_depth': 80,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.01,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.01,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.01,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.01,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.01,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.01,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.01,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.01,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.01,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.01,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.01,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.01,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.01,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.01,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.01,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.01,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.01,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.01,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.01,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.01,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 0.1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 1,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 1,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                 beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                 hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                 learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "                 momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "                 power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "                 tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "                 warm_start=False),\n",
       "   'model__alpha': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0.5,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0.5,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0.5,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0.5,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0.5,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0.5,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0.5,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0.5,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0.5,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0.5,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0.5,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0.5,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0.5,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0.5,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0.5,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0.5,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0.5,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0.5,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0.5,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 0.5,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 1.0,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 1.0,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 1.0,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 1.0,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 1.0,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 1.0,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 1.0,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 1.0,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 1.0,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 1.0,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 1.0,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 1.0,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 1.0,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 1.0,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 1.0,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 1.0,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 1.0,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 1.0,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 1.0,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 1.0,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'model__alpha': 10,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 100,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 100,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 150,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 150,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 150,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 150,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 150,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 150,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 150,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 150,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 150,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 150,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 150,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 150,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 150,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 150,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 150,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 150,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 150,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 150,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 150,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 150,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 200,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 200,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 200,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 200,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 200,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 200,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 200,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 200,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 200,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 200,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 200,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 200,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 200,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 200,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 200,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 200,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 200,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 200,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 200,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 200,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 250,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 250,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 250,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 250,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 250,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 250,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 250,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 250,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 250,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 250,\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 250,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 250,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 250,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 250,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 250,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 250,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 250,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 250,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 250,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                      n_estimators=50, random_state=None),\n",
       "   'model__n_estimators': 250,\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "             verbose=0),\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "             verbose=0),\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "             verbose=0),\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "             verbose=0),\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "             verbose=0),\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "             verbose=0),\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "             verbose=0),\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "             verbose=0),\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "             verbose=0),\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "             verbose=0),\n",
       "   'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "             verbose=0),\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "             verbose=0),\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 100,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "             verbose=0),\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "             verbose=0),\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 200,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "             verbose=0),\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "             verbose=0),\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 300,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "             verbose=0),\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "             verbose=0),\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 400,\n",
       "   'vectorizer__ngram_range': (1, 3)},\n",
       "  {'model': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "             verbose=0),\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 2)},\n",
       "  {'model': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "             verbose=0),\n",
       "   'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=100, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True,\n",
       "                   stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                               'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                               \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                               'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                               'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                               'itself', ...],\n",
       "                   strip_accents=None, sublinear_tf=False,\n",
       "                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function my_lemmatization_tokenizer at 0x7f9588c81c80>,\n",
       "                   use_idf=True, vocabulary=None),\n",
       "   'vectorizer__min_df': 500,\n",
       "   'vectorizer__ngram_range': (1, 3)}],\n",
       " 'split0_test_score': array([0.89673913, 0.89673913, 0.7173913 , 0.7173913 , 0.65217391,\n",
       "        0.65217391,        nan,        nan,        nan,        nan,\n",
       "        0.89130435, 0.89130435, 0.72826087, 0.72826087, 0.67391304,\n",
       "        0.67391304,        nan,        nan,        nan,        nan,\n",
       "        0.91304348, 0.91304348, 0.70652174, 0.70652174, 0.6576087 ,\n",
       "        0.6576087 ,        nan,        nan,        nan,        nan,\n",
       "        0.89673913, 0.89673913, 0.73913043, 0.73913043, 0.66304348,\n",
       "        0.66304348,        nan,        nan,        nan,        nan,\n",
       "        0.9076087 , 0.9076087 , 0.70108696, 0.70108696, 0.64673913,\n",
       "        0.64673913,        nan,        nan,        nan,        nan,\n",
       "        0.91304348, 0.91304348, 0.73913043, 0.73913043, 0.6576087 ,\n",
       "        0.6576087 ,        nan,        nan,        nan,        nan,\n",
       "        0.875     , 0.875     , 0.70108696, 0.70108696, 0.64673913,\n",
       "        0.64673913,        nan,        nan,        nan,        nan,\n",
       "        0.92934783, 0.92934783, 0.73369565, 0.73369565, 0.6576087 ,\n",
       "        0.6576087 ,        nan,        nan,        nan,        nan,\n",
       "        0.875     , 0.875     , 0.70108696, 0.70108696, 0.64673913,\n",
       "        0.64673913,        nan,        nan,        nan,        nan,\n",
       "        0.9076087 , 0.9076087 , 0.73369565, 0.73369565, 0.6576087 ,\n",
       "        0.6576087 ,        nan,        nan,        nan,        nan,\n",
       "        0.875     , 0.875     , 0.70108696, 0.70108696, 0.64673913,\n",
       "        0.64673913,        nan,        nan,        nan,        nan,\n",
       "        0.9076087 , 0.9076087 , 0.73369565, 0.73369565, 0.6576087 ,\n",
       "        0.6576087 ,        nan,        nan,        nan,        nan,\n",
       "        0.8423913 , 0.8423913 , 0.67391304, 0.6576087 , 0.5923913 ,\n",
       "        0.58695652,        nan,        nan,        nan,        nan,\n",
       "        0.79891304, 0.79891304, 0.68478261, 0.68478261, 0.61956522,\n",
       "        0.61956522,        nan,        nan,        nan,        nan,\n",
       "        0.82065217, 0.86413043, 0.65217391, 0.64673913, 0.58152174,\n",
       "        0.58695652,        nan,        nan,        nan,        nan,\n",
       "        0.82608696, 0.83695652, 0.65217391, 0.6576087 , 0.60869565,\n",
       "        0.61413043,        nan,        nan,        nan,        nan,\n",
       "        0.81521739, 0.8423913 , 0.65217391, 0.67934783, 0.58695652,\n",
       "        0.5923913 ,        nan,        nan,        nan,        nan,\n",
       "        0.83152174, 0.82065217, 0.6576087 , 0.6576087 , 0.59782609,\n",
       "        0.61956522,        nan,        nan,        nan,        nan,\n",
       "        0.82608696, 0.8423913 , 0.66304348, 0.65217391, 0.58695652,\n",
       "        0.58695652,        nan,        nan,        nan,        nan,\n",
       "        0.83695652, 0.82608696, 0.65217391, 0.66304348, 0.60869565,\n",
       "        0.59782609,        nan,        nan,        nan,        nan,\n",
       "        0.92391304, 0.92391304, 0.73913043, 0.72282609, 0.61956522,\n",
       "        0.61413043,        nan,        nan,        nan,        nan,\n",
       "        0.91847826, 0.91304348, 0.70652174, 0.7173913 , 0.63586957,\n",
       "        0.63043478,        nan,        nan,        nan,        nan,\n",
       "        0.91847826, 0.93478261, 0.70108696, 0.73913043, 0.61956522,\n",
       "        0.61956522,        nan,        nan,        nan,        nan,\n",
       "        0.91847826, 0.91304348, 0.71195652, 0.70108696, 0.64130435,\n",
       "        0.63586957,        nan,        nan,        nan,        nan,\n",
       "        0.94021739, 0.91847826, 0.7173913 , 0.72282609, 0.61413043,\n",
       "        0.63043478,        nan,        nan,        nan,        nan,\n",
       "        0.94021739, 0.92391304, 0.71195652, 0.70108696, 0.64130435,\n",
       "        0.63586957,        nan,        nan,        nan,        nan,\n",
       "        0.92391304, 0.91847826, 0.69565217, 0.73913043, 0.625     ,\n",
       "        0.63043478,        nan,        nan,        nan,        nan,\n",
       "        0.93478261, 0.92391304, 0.70652174, 0.68478261, 0.63586957,\n",
       "        0.64130435,        nan,        nan,        nan,        nan,\n",
       "        0.91847826, 0.9076087 , 0.69565217, 0.67391304, 0.69565217,\n",
       "        0.69565217,        nan,        nan,        nan,        nan,\n",
       "        0.90217391, 0.91304348, 0.7173913 , 0.71195652, 0.67934783,\n",
       "        0.67391304,        nan,        nan,        nan,        nan,\n",
       "        0.91304348, 0.91304348, 0.7173913 , 0.6576087 , 0.67934783,\n",
       "        0.66304348,        nan,        nan,        nan,        nan,\n",
       "        0.92391304, 0.92391304, 0.7173913 , 0.72282609, 0.67391304,\n",
       "        0.67391304,        nan,        nan,        nan,        nan,\n",
       "        0.91304348, 0.91304348, 0.72826087, 0.72282609, 0.67391304,\n",
       "        0.67391304,        nan,        nan,        nan,        nan,\n",
       "        0.92934783, 0.92934783, 0.7173913 , 0.72826087, 0.67934783,\n",
       "        0.66847826,        nan,        nan,        nan,        nan,\n",
       "        0.91304348, 0.9076087 , 0.72826087, 0.7173913 , 0.66847826,\n",
       "        0.66847826,        nan,        nan,        nan,        nan,\n",
       "        0.90217391, 0.90217391, 0.73369565, 0.73913043, 0.67391304,\n",
       "        0.67391304,        nan,        nan,        nan,        nan,\n",
       "        0.83152174, 0.82608696, 0.50543478, 0.61413043, 0.50543478,\n",
       "        0.5       ,        nan,        nan,        nan,        nan,\n",
       "        0.50543478, 0.50543478, 0.50543478, 0.50543478, 0.50543478,\n",
       "        0.49456522,        nan,        nan,        nan,        nan,\n",
       "        0.90217391, 0.90217391, 0.74456522, 0.74456522, 0.67391304,\n",
       "        0.67391304,        nan,        nan,        nan,        nan,\n",
       "        0.9076087 , 0.9076087 , 0.75      , 0.75      , 0.66304348,\n",
       "        0.66304348,        nan,        nan,        nan,        nan,\n",
       "        0.90217391, 0.90217391, 0.74456522, 0.74456522, 0.67391304,\n",
       "        0.67391304,        nan,        nan,        nan,        nan,\n",
       "        0.90217391, 0.90217391, 0.75      , 0.75      , 0.66304348,\n",
       "        0.66304348,        nan,        nan,        nan,        nan,\n",
       "        0.90217391, 0.90217391, 0.74456522, 0.74456522, 0.67391304,\n",
       "        0.67391304,        nan,        nan,        nan,        nan,\n",
       "        0.90217391, 0.90217391, 0.75      , 0.75      , 0.66304348,\n",
       "        0.66304348,        nan,        nan,        nan,        nan,\n",
       "        0.89673913, 0.89673913, 0.74456522, 0.74456522, 0.67391304,\n",
       "        0.67391304,        nan,        nan,        nan,        nan,\n",
       "        0.89673913, 0.89673913, 0.76630435, 0.76630435, 0.66304348,\n",
       "        0.66304348,        nan,        nan,        nan,        nan,\n",
       "        0.91847826, 0.91847826, 0.73913043, 0.73913043, 0.64130435,\n",
       "        0.64130435,        nan,        nan,        nan,        nan,\n",
       "        0.875     , 0.89130435, 0.69021739, 0.69021739, 0.63043478,\n",
       "        0.63043478,        nan,        nan,        nan,        nan,\n",
       "        0.93478261, 0.93478261, 0.7173913 , 0.7173913 , 0.64130435,\n",
       "        0.64130435,        nan,        nan,        nan,        nan,\n",
       "        0.89673913, 0.89673913, 0.65217391, 0.65217391, 0.63586957,\n",
       "        0.63586957,        nan,        nan,        nan,        nan,\n",
       "        0.91847826, 0.91847826, 0.72826087, 0.72826087, 0.63586957,\n",
       "        0.63586957,        nan,        nan,        nan,        nan,\n",
       "        0.90217391, 0.90217391, 0.63043478, 0.63043478, 0.63043478,\n",
       "        0.63043478,        nan,        nan,        nan,        nan,\n",
       "        0.9076087 , 0.9076087 , 0.72826087, 0.72826087, 0.64130435,\n",
       "        0.64130435,        nan,        nan,        nan,        nan,\n",
       "        0.90217391, 0.88586957, 0.6576087 , 0.6576087 , 0.61413043,\n",
       "        0.61413043,        nan,        nan,        nan,        nan,\n",
       "        0.88043478, 0.88586957, 0.71195652, 0.71195652, 0.67391304,\n",
       "        0.67391304,        nan,        nan,        nan,        nan,\n",
       "        0.92934783, 0.92934783, 0.72826087, 0.72826087, 0.6576087 ,\n",
       "        0.6576087 ,        nan,        nan,        nan,        nan]),\n",
       " 'split1_test_score': array([0.83695652, 0.83695652, 0.61956522, 0.61956522, 0.58695652,\n",
       "        0.58695652,        nan,        nan,        nan,        nan,\n",
       "        0.85869565, 0.85869565, 0.63586957, 0.63586957, 0.60869565,\n",
       "        0.60869565,        nan,        nan,        nan,        nan,\n",
       "        0.85869565, 0.85869565, 0.61956522, 0.61956522, 0.60869565,\n",
       "        0.60869565,        nan,        nan,        nan,        nan,\n",
       "        0.85326087, 0.85326087, 0.63586957, 0.63586957, 0.61956522,\n",
       "        0.61956522,        nan,        nan,        nan,        nan,\n",
       "        0.84782609, 0.84782609, 0.61956522, 0.61956522, 0.61413043,\n",
       "        0.61413043,        nan,        nan,        nan,        nan,\n",
       "        0.86413043, 0.86413043, 0.625     , 0.625     , 0.61413043,\n",
       "        0.61413043,        nan,        nan,        nan,        nan,\n",
       "        0.84782609, 0.84782609, 0.625     , 0.625     , 0.61413043,\n",
       "        0.61413043,        nan,        nan,        nan,        nan,\n",
       "        0.86413043, 0.86413043, 0.61413043, 0.61413043, 0.61413043,\n",
       "        0.61413043,        nan,        nan,        nan,        nan,\n",
       "        0.83152174, 0.83152174, 0.625     , 0.625     , 0.61413043,\n",
       "        0.61413043,        nan,        nan,        nan,        nan,\n",
       "        0.85869565, 0.85869565, 0.61413043, 0.61413043, 0.61413043,\n",
       "        0.61413043,        nan,        nan,        nan,        nan,\n",
       "        0.83152174, 0.83152174, 0.625     , 0.625     , 0.61413043,\n",
       "        0.61413043,        nan,        nan,        nan,        nan,\n",
       "        0.85869565, 0.85869565, 0.61413043, 0.61413043, 0.61413043,\n",
       "        0.61413043,        nan,        nan,        nan,        nan,\n",
       "        0.76630435, 0.77717391, 0.56521739, 0.55978261, 0.52717391,\n",
       "        0.50543478,        nan,        nan,        nan,        nan,\n",
       "        0.77173913, 0.77173913, 0.61413043, 0.5923913 , 0.53804348,\n",
       "        0.54347826,        nan,        nan,        nan,        nan,\n",
       "        0.76086957, 0.75543478, 0.58695652, 0.56521739, 0.51630435,\n",
       "        0.51630435,        nan,        nan,        nan,        nan,\n",
       "        0.76630435, 0.76630435, 0.59782609, 0.5923913 , 0.54891304,\n",
       "        0.54347826,        nan,        nan,        nan,        nan,\n",
       "        0.79891304, 0.78804348, 0.55978261, 0.55434783, 0.52173913,\n",
       "        0.51086957,        nan,        nan,        nan,        nan,\n",
       "        0.76086957, 0.76630435, 0.58695652, 0.59782609, 0.54891304,\n",
       "        0.5326087 ,        nan,        nan,        nan,        nan,\n",
       "        0.76086957, 0.75543478, 0.55978261, 0.55434783, 0.53804348,\n",
       "        0.52173913,        nan,        nan,        nan,        nan,\n",
       "        0.77717391, 0.75543478, 0.60326087, 0.60869565, 0.53804348,\n",
       "        0.5326087 ,        nan,        nan,        nan,        nan,\n",
       "        0.90217391, 0.89673913, 0.63586957, 0.625     , 0.54347826,\n",
       "        0.56521739,        nan,        nan,        nan,        nan,\n",
       "        0.86956522, 0.86956522, 0.63043478, 0.60326087, 0.55978261,\n",
       "        0.55434783,        nan,        nan,        nan,        nan,\n",
       "        0.88043478, 0.88586957, 0.61413043, 0.64130435, 0.54347826,\n",
       "        0.54347826,        nan,        nan,        nan,        nan,\n",
       "        0.86413043, 0.88586957, 0.61956522, 0.625     , 0.57065217,\n",
       "        0.55978261,        nan,        nan,        nan,        nan,\n",
       "        0.86956522, 0.86956522, 0.63586957, 0.64130435, 0.54347826,\n",
       "        0.54891304,        nan,        nan,        nan,        nan,\n",
       "        0.85326087, 0.86956522, 0.60326087, 0.625     , 0.55434783,\n",
       "        0.55978261,        nan,        nan,        nan,        nan,\n",
       "        0.875     , 0.86413043, 0.64130435, 0.63043478, 0.55434783,\n",
       "        0.5326087 ,        nan,        nan,        nan,        nan,\n",
       "        0.86413043, 0.875     , 0.60869565, 0.61413043, 0.54891304,\n",
       "        0.55978261,        nan,        nan,        nan,        nan,\n",
       "        0.88586957, 0.85869565, 0.63043478, 0.63586957, 0.61413043,\n",
       "        0.625     ,        nan,        nan,        nan,        nan,\n",
       "        0.84782609, 0.85326087, 0.63043478, 0.63586957, 0.61956522,\n",
       "        0.625     ,        nan,        nan,        nan,        nan,\n",
       "        0.88043478, 0.875     , 0.61956522, 0.64130435, 0.625     ,\n",
       "        0.61956522,        nan,        nan,        nan,        nan,\n",
       "        0.85869565, 0.85326087, 0.65217391, 0.60326087, 0.61413043,\n",
       "        0.61413043,        nan,        nan,        nan,        nan,\n",
       "        0.88043478, 0.88043478, 0.61956522, 0.61956522, 0.61956522,\n",
       "        0.60869565,        nan,        nan,        nan,        nan,\n",
       "        0.86413043, 0.86956522, 0.61956522, 0.625     , 0.61413043,\n",
       "        0.63043478,        nan,        nan,        nan,        nan,\n",
       "        0.85326087, 0.86413043, 0.625     , 0.625     , 0.61413043,\n",
       "        0.61413043,        nan,        nan,        nan,        nan,\n",
       "        0.86413043, 0.86413043, 0.63586957, 0.63586957, 0.60869565,\n",
       "        0.60869565,        nan,        nan,        nan,        nan,\n",
       "        0.77173913, 0.77173913, 0.49456522, 0.56521739, 0.51086957,\n",
       "        0.5       ,        nan,        nan,        nan,        nan,\n",
       "        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.5       ,        nan,        nan,        nan,        nan,\n",
       "        0.86956522, 0.86956522, 0.61956522, 0.61956522, 0.60869565,\n",
       "        0.60869565,        nan,        nan,        nan,        nan,\n",
       "        0.86413043, 0.86413043, 0.61413043, 0.61413043, 0.60326087,\n",
       "        0.60326087,        nan,        nan,        nan,        nan,\n",
       "        0.86956522, 0.86956522, 0.61956522, 0.61956522, 0.60869565,\n",
       "        0.60869565,        nan,        nan,        nan,        nan,\n",
       "        0.86413043, 0.86413043, 0.61413043, 0.61413043, 0.60869565,\n",
       "        0.60869565,        nan,        nan,        nan,        nan,\n",
       "        0.86956522, 0.86956522, 0.61956522, 0.61956522, 0.60869565,\n",
       "        0.60869565,        nan,        nan,        nan,        nan,\n",
       "        0.86413043, 0.86413043, 0.61413043, 0.61413043, 0.60869565,\n",
       "        0.60869565,        nan,        nan,        nan,        nan,\n",
       "        0.86413043, 0.86413043, 0.61956522, 0.61956522, 0.60869565,\n",
       "        0.60869565,        nan,        nan,        nan,        nan,\n",
       "        0.85326087, 0.85326087, 0.60869565, 0.60869565, 0.60869565,\n",
       "        0.60869565,        nan,        nan,        nan,        nan,\n",
       "        0.86956522, 0.86956522, 0.64130435, 0.64130435, 0.56521739,\n",
       "        0.56521739,        nan,        nan,        nan,        nan,\n",
       "        0.81521739, 0.81521739, 0.56521739, 0.56521739, 0.57065217,\n",
       "        0.57065217,        nan,        nan,        nan,        nan,\n",
       "        0.85326087, 0.85326087, 0.63043478, 0.63043478, 0.56521739,\n",
       "        0.56521739,        nan,        nan,        nan,        nan,\n",
       "        0.86413043, 0.86413043, 0.58152174, 0.58152174, 0.57608696,\n",
       "        0.57608696,        nan,        nan,        nan,        nan,\n",
       "        0.85326087, 0.85326087, 0.625     , 0.625     , 0.56521739,\n",
       "        0.56521739,        nan,        nan,        nan,        nan,\n",
       "        0.8423913 , 0.8423913 , 0.5923913 , 0.5923913 , 0.55978261,\n",
       "        0.55978261,        nan,        nan,        nan,        nan,\n",
       "        0.83695652, 0.83695652, 0.63586957, 0.63586957, 0.55978261,\n",
       "        0.55978261,        nan,        nan,        nan,        nan,\n",
       "        0.83695652, 0.83695652, 0.59782609, 0.59782609, 0.56521739,\n",
       "        0.56521739,        nan,        nan,        nan,        nan,\n",
       "        0.83695652, 0.84782609, 0.625     , 0.625     , 0.61413043,\n",
       "        0.61413043,        nan,        nan,        nan,        nan,\n",
       "        0.86956522, 0.86956522, 0.61956522, 0.61956522, 0.61956522,\n",
       "        0.61956522,        nan,        nan,        nan,        nan]),\n",
       " 'split2_test_score': array([0.87978142, 0.87978142, 0.72677596, 0.72677596, 0.69945355,\n",
       "        0.69945355,        nan,        nan,        nan,        nan,\n",
       "        0.90163934, 0.90163934, 0.72677596, 0.72677596, 0.70491803,\n",
       "        0.70491803,        nan,        nan,        nan,        nan,\n",
       "        0.90163934, 0.90163934, 0.71584699, 0.71584699, 0.66120219,\n",
       "        0.66120219,        nan,        nan,        nan,        nan,\n",
       "        0.89071038, 0.89071038, 0.72677596, 0.72677596, 0.70491803,\n",
       "        0.70491803,        nan,        nan,        nan,        nan,\n",
       "        0.8579235 , 0.8579235 , 0.72677596, 0.72677596, 0.66120219,\n",
       "        0.66120219,        nan,        nan,        nan,        nan,\n",
       "        0.91256831, 0.91256831, 0.72131148, 0.72131148, 0.69398907,\n",
       "        0.69398907,        nan,        nan,        nan,        nan,\n",
       "        0.84699454, 0.84699454, 0.72677596, 0.72677596, 0.66120219,\n",
       "        0.66120219,        nan,        nan,        nan,        nan,\n",
       "        0.90710383, 0.90710383, 0.72131148, 0.72131148, 0.69398907,\n",
       "        0.69398907,        nan,        nan,        nan,        nan,\n",
       "        0.84153005, 0.84153005, 0.72677596, 0.72677596, 0.66120219,\n",
       "        0.66120219,        nan,        nan,        nan,        nan,\n",
       "        0.91256831, 0.91256831, 0.72677596, 0.72677596, 0.69398907,\n",
       "        0.69398907,        nan,        nan,        nan,        nan,\n",
       "        0.84153005, 0.84153005, 0.72677596, 0.72677596, 0.66120219,\n",
       "        0.66120219,        nan,        nan,        nan,        nan,\n",
       "        0.91256831, 0.91256831, 0.72677596, 0.72677596, 0.69398907,\n",
       "        0.69398907,        nan,        nan,        nan,        nan,\n",
       "        0.81420765, 0.81967213, 0.66666667, 0.64480874, 0.60655738,\n",
       "        0.58469945,        nan,        nan,        nan,        nan,\n",
       "        0.82513661, 0.84153005, 0.67213115, 0.66120219, 0.59562842,\n",
       "        0.6010929 ,        nan,        nan,        nan,        nan,\n",
       "        0.82513661, 0.84153005, 0.68852459, 0.6557377 , 0.58469945,\n",
       "        0.57923497,        nan,        nan,        nan,        nan,\n",
       "        0.80874317, 0.83060109, 0.66666667, 0.6557377 , 0.60655738,\n",
       "        0.60655738,        nan,        nan,        nan,        nan,\n",
       "        0.84699454, 0.81967213, 0.67759563, 0.67213115, 0.6010929 ,\n",
       "        0.57377049,        nan,        nan,        nan,        nan,\n",
       "        0.83060109, 0.83606557, 0.66666667, 0.66120219, 0.60655738,\n",
       "        0.60655738,        nan,        nan,        nan,        nan,\n",
       "        0.79781421, 0.82513661, 0.6557377 , 0.66120219, 0.57377049,\n",
       "        0.57377049,        nan,        nan,        nan,        nan,\n",
       "        0.82513661, 0.85245902, 0.6557377 , 0.67213115, 0.60655738,\n",
       "        0.59562842,        nan,        nan,        nan,        nan,\n",
       "        0.91256831, 0.8852459 , 0.70491803, 0.71038251, 0.61202186,\n",
       "        0.63934426,        nan,        nan,        nan,        nan,\n",
       "        0.89617486, 0.92896175, 0.73224044, 0.73224044, 0.62295082,\n",
       "        0.64480874,        nan,        nan,        nan,        nan,\n",
       "        0.89071038, 0.90710383, 0.71038251, 0.72131148, 0.6284153 ,\n",
       "        0.61202186,        nan,        nan,        nan,        nan,\n",
       "        0.91803279, 0.92349727, 0.74863388, 0.72677596, 0.6284153 ,\n",
       "        0.62295082,        nan,        nan,        nan,        nan,\n",
       "        0.90710383, 0.90710383, 0.70491803, 0.71584699, 0.62295082,\n",
       "        0.62295082,        nan,        nan,        nan,        nan,\n",
       "        0.90710383, 0.91803279, 0.73770492, 0.72131148, 0.6284153 ,\n",
       "        0.6284153 ,        nan,        nan,        nan,        nan,\n",
       "        0.90710383, 0.91256831, 0.70491803, 0.69945355, 0.60655738,\n",
       "        0.62295082,        nan,        nan,        nan,        nan,\n",
       "        0.92349727, 0.91256831, 0.72131148, 0.73224044, 0.63934426,\n",
       "        0.63387978,        nan,        nan,        nan,        nan,\n",
       "        0.90710383, 0.89617486, 0.71584699, 0.71038251, 0.65027322,\n",
       "        0.67213115,        nan,        nan,        nan,        nan,\n",
       "        0.89617486, 0.89617486, 0.7431694 , 0.75409836, 0.69398907,\n",
       "        0.69398907,        nan,        nan,        nan,        nan,\n",
       "        0.91803279, 0.91256831, 0.73224044, 0.7431694 , 0.67213115,\n",
       "        0.6557377 ,        nan,        nan,        nan,        nan,\n",
       "        0.90710383, 0.90710383, 0.7431694 , 0.7431694 , 0.69945355,\n",
       "        0.70491803,        nan,        nan,        nan,        nan,\n",
       "        0.91256831, 0.91256831, 0.73770492, 0.7431694 , 0.68852459,\n",
       "        0.69398907,        nan,        nan,        nan,        nan,\n",
       "        0.91803279, 0.90163934, 0.73224044, 0.73224044, 0.69945355,\n",
       "        0.69398907,        nan,        nan,        nan,        nan,\n",
       "        0.90163934, 0.91803279, 0.71038251, 0.71038251, 0.70491803,\n",
       "        0.68852459,        nan,        nan,        nan,        nan,\n",
       "        0.89071038, 0.8852459 , 0.72131148, 0.73224044, 0.70491803,\n",
       "        0.70491803,        nan,        nan,        nan,        nan,\n",
       "        0.84699454, 0.85245902, 0.49726776, 0.50273224, 0.49726776,\n",
       "        0.50273224,        nan,        nan,        nan,        nan,\n",
       "        0.50273224, 0.49726776, 0.50273224, 0.49726776, 0.50273224,\n",
       "        0.49726776,        nan,        nan,        nan,        nan,\n",
       "        0.89071038, 0.89071038, 0.71584699, 0.71584699, 0.68852459,\n",
       "        0.68852459,        nan,        nan,        nan,        nan,\n",
       "        0.91256831, 0.91256831, 0.71038251, 0.71038251, 0.70491803,\n",
       "        0.70491803,        nan,        nan,        nan,        nan,\n",
       "        0.89071038, 0.89071038, 0.71584699, 0.71584699, 0.68852459,\n",
       "        0.68852459,        nan,        nan,        nan,        nan,\n",
       "        0.91256831, 0.91256831, 0.71038251, 0.71038251, 0.70491803,\n",
       "        0.70491803,        nan,        nan,        nan,        nan,\n",
       "        0.89071038, 0.89071038, 0.71584699, 0.71584699, 0.68852459,\n",
       "        0.68852459,        nan,        nan,        nan,        nan,\n",
       "        0.90710383, 0.90710383, 0.71038251, 0.71038251, 0.70491803,\n",
       "        0.70491803,        nan,        nan,        nan,        nan,\n",
       "        0.89071038, 0.89071038, 0.71038251, 0.71038251, 0.68852459,\n",
       "        0.68852459,        nan,        nan,        nan,        nan,\n",
       "        0.8852459 , 0.8852459 , 0.70491803, 0.70491803, 0.69945355,\n",
       "        0.69945355,        nan,        nan,        nan,        nan,\n",
       "        0.87978142, 0.87978142, 0.71038251, 0.71038251, 0.68852459,\n",
       "        0.68852459,        nan,        nan,        nan,        nan,\n",
       "        0.87978142, 0.87978142, 0.68306011, 0.68306011, 0.67759563,\n",
       "        0.67759563,        nan,        nan,        nan,        nan,\n",
       "        0.86885246, 0.86885246, 0.72131148, 0.72131148, 0.70491803,\n",
       "        0.70491803,        nan,        nan,        nan,        nan,\n",
       "        0.8852459 , 0.8852459 , 0.68852459, 0.68852459, 0.67213115,\n",
       "        0.67213115,        nan,        nan,        nan,        nan,\n",
       "        0.85245902, 0.85245902, 0.72677596, 0.72677596, 0.70491803,\n",
       "        0.70491803,        nan,        nan,        nan,        nan,\n",
       "        0.90163934, 0.90163934, 0.72131148, 0.72131148, 0.66666667,\n",
       "        0.66666667,        nan,        nan,        nan,        nan,\n",
       "        0.83060109, 0.83060109, 0.73224044, 0.73224044, 0.71038251,\n",
       "        0.71038251,        nan,        nan,        nan,        nan,\n",
       "        0.90163934, 0.90163934, 0.70491803, 0.70491803, 0.67213115,\n",
       "        0.67213115,        nan,        nan,        nan,        nan,\n",
       "        0.85245902, 0.85245902, 0.73224044, 0.71584699, 0.69398907,\n",
       "        0.69398907,        nan,        nan,        nan,        nan,\n",
       "        0.90710383, 0.90710383, 0.72131148, 0.72131148, 0.69945355,\n",
       "        0.69945355,        nan,        nan,        nan,        nan]),\n",
       " 'split3_test_score': array([0.92349727, 0.92349727, 0.71584699, 0.71584699, 0.6557377 ,\n",
       "        0.6557377 ,        nan,        nan,        nan,        nan,\n",
       "        0.90710383, 0.90710383, 0.72131148, 0.72131148, 0.63934426,\n",
       "        0.63934426,        nan,        nan,        nan,        nan,\n",
       "        0.92349727, 0.92349727, 0.70491803, 0.70491803, 0.64480874,\n",
       "        0.64480874,        nan,        nan,        nan,        nan,\n",
       "        0.92896175, 0.92896175, 0.74863388, 0.74863388, 0.63934426,\n",
       "        0.63934426,        nan,        nan,        nan,        nan,\n",
       "        0.92349727, 0.92349727, 0.70491803, 0.70491803, 0.6284153 ,\n",
       "        0.6284153 ,        nan,        nan,        nan,        nan,\n",
       "        0.91803279, 0.91803279, 0.73224044, 0.73224044, 0.63387978,\n",
       "        0.63387978,        nan,        nan,        nan,        nan,\n",
       "        0.89617486, 0.89617486, 0.70491803, 0.70491803, 0.6284153 ,\n",
       "        0.6284153 ,        nan,        nan,        nan,        nan,\n",
       "        0.92896175, 0.92896175, 0.73770492, 0.73770492, 0.63387978,\n",
       "        0.63387978,        nan,        nan,        nan,        nan,\n",
       "        0.89617486, 0.89617486, 0.70491803, 0.70491803, 0.6284153 ,\n",
       "        0.6284153 ,        nan,        nan,        nan,        nan,\n",
       "        0.91256831, 0.91256831, 0.73770492, 0.73770492, 0.63387978,\n",
       "        0.63387978,        nan,        nan,        nan,        nan,\n",
       "        0.89617486, 0.89617486, 0.70491803, 0.70491803, 0.6284153 ,\n",
       "        0.6284153 ,        nan,        nan,        nan,        nan,\n",
       "        0.91256831, 0.91256831, 0.73770492, 0.73770492, 0.63387978,\n",
       "        0.63387978,        nan,        nan,        nan,        nan,\n",
       "        0.80874317, 0.82513661, 0.62295082, 0.63934426, 0.53551913,\n",
       "        0.54098361,        nan,        nan,        nan,        nan,\n",
       "        0.82513661, 0.83060109, 0.70491803, 0.71584699, 0.55191257,\n",
       "        0.55191257,        nan,        nan,        nan,        nan,\n",
       "        0.81967213, 0.80874317, 0.64480874, 0.6284153 , 0.54644809,\n",
       "        0.55191257,        nan,        nan,        nan,        nan,\n",
       "        0.80327869, 0.84699454, 0.71038251, 0.71584699, 0.54644809,\n",
       "        0.56284153,        nan,        nan,        nan,        nan,\n",
       "        0.80874317, 0.81967213, 0.63387978, 0.6284153 , 0.54098361,\n",
       "        0.55191257,        nan,        nan,        nan,        nan,\n",
       "        0.81420765, 0.80327869, 0.71038251, 0.71038251, 0.55737705,\n",
       "        0.55191257,        nan,        nan,        nan,        nan,\n",
       "        0.82513661, 0.81420765, 0.61748634, 0.63387978, 0.53551913,\n",
       "        0.55737705,        nan,        nan,        nan,        nan,\n",
       "        0.81967213, 0.84153005, 0.72131148, 0.71038251, 0.54644809,\n",
       "        0.55737705,        nan,        nan,        nan,        nan,\n",
       "        0.92349727, 0.91803279, 0.72677596, 0.74863388, 0.57923497,\n",
       "        0.59562842,        nan,        nan,        nan,        nan,\n",
       "        0.93442623, 0.92349727, 0.73224044, 0.74863388, 0.6010929 ,\n",
       "        0.59016393,        nan,        nan,        nan,        nan,\n",
       "        0.91803279, 0.92896175, 0.72677596, 0.73770492, 0.58469945,\n",
       "        0.59562842,        nan,        nan,        nan,        nan,\n",
       "        0.92896175, 0.94535519, 0.7431694 , 0.73224044, 0.58469945,\n",
       "        0.59562842,        nan,        nan,        nan,        nan,\n",
       "        0.93442623, 0.92896175, 0.71038251, 0.76502732, 0.59562842,\n",
       "        0.58469945,        nan,        nan,        nan,        nan,\n",
       "        0.92349727, 0.93442623, 0.71584699, 0.75409836, 0.57377049,\n",
       "        0.61748634,        nan,        nan,        nan,        nan,\n",
       "        0.92349727, 0.91256831, 0.75409836, 0.72677596, 0.6010929 ,\n",
       "        0.59016393,        nan,        nan,        nan,        nan,\n",
       "        0.92896175, 0.93442623, 0.73770492, 0.72131148, 0.59016393,\n",
       "        0.59016393,        nan,        nan,        nan,        nan,\n",
       "        0.90710383, 0.89617486, 0.74863388, 0.74863388, 0.6284153 ,\n",
       "        0.62295082,        nan,        nan,        nan,        nan,\n",
       "        0.90163934, 0.90163934, 0.73770492, 0.73224044, 0.63934426,\n",
       "        0.63934426,        nan,        nan,        nan,        nan,\n",
       "        0.89071038, 0.91256831, 0.7431694 , 0.7431694 , 0.63934426,\n",
       "        0.6284153 ,        nan,        nan,        nan,        nan,\n",
       "        0.91803279, 0.91256831, 0.73770492, 0.73770492, 0.64480874,\n",
       "        0.63387978,        nan,        nan,        nan,        nan,\n",
       "        0.92349727, 0.91256831, 0.7704918 , 0.7431694 , 0.6284153 ,\n",
       "        0.63387978,        nan,        nan,        nan,        nan,\n",
       "        0.92349727, 0.92349727, 0.73770492, 0.72677596, 0.63387978,\n",
       "        0.63387978,        nan,        nan,        nan,        nan,\n",
       "        0.91803279, 0.92349727, 0.73770492, 0.72677596, 0.6557377 ,\n",
       "        0.6557377 ,        nan,        nan,        nan,        nan,\n",
       "        0.91803279, 0.91803279, 0.72677596, 0.71038251, 0.63934426,\n",
       "        0.6557377 ,        nan,        nan,        nan,        nan,\n",
       "        0.81420765, 0.79781421, 0.50273224, 0.50273224, 0.49726776,\n",
       "        0.57923497,        nan,        nan,        nan,        nan,\n",
       "        0.50273224, 0.50273224, 0.50273224, 0.49726776, 0.50273224,\n",
       "        0.50273224,        nan,        nan,        nan,        nan,\n",
       "        0.94535519, 0.94535519, 0.75409836, 0.75409836, 0.63387978,\n",
       "        0.63387978,        nan,        nan,        nan,        nan,\n",
       "        0.94535519, 0.94535519, 0.77595628, 0.77595628, 0.63934426,\n",
       "        0.63934426,        nan,        nan,        nan,        nan,\n",
       "        0.94535519, 0.94535519, 0.75409836, 0.75409836, 0.63387978,\n",
       "        0.63387978,        nan,        nan,        nan,        nan,\n",
       "        0.94535519, 0.94535519, 0.77595628, 0.77595628, 0.63934426,\n",
       "        0.63934426,        nan,        nan,        nan,        nan,\n",
       "        0.94535519, 0.94535519, 0.75409836, 0.75409836, 0.63387978,\n",
       "        0.63387978,        nan,        nan,        nan,        nan,\n",
       "        0.93989071, 0.93989071, 0.7704918 , 0.7704918 , 0.63934426,\n",
       "        0.63934426,        nan,        nan,        nan,        nan,\n",
       "        0.93989071, 0.93989071, 0.74863388, 0.74863388, 0.63387978,\n",
       "        0.63387978,        nan,        nan,        nan,        nan,\n",
       "        0.93442623, 0.93442623, 0.78688525, 0.78688525, 0.63934426,\n",
       "        0.63934426,        nan,        nan,        nan,        nan,\n",
       "        0.8852459 , 0.8852459 , 0.73224044, 0.73224044, 0.64480874,\n",
       "        0.64480874,        nan,        nan,        nan,        nan,\n",
       "        0.89617486, 0.89617486, 0.72131148, 0.72131148, 0.65027322,\n",
       "        0.65027322,        nan,        nan,        nan,        nan,\n",
       "        0.8852459 , 0.8852459 , 0.73224044, 0.73224044, 0.63387978,\n",
       "        0.63387978,        nan,        nan,        nan,        nan,\n",
       "        0.89071038, 0.89071038, 0.7431694 , 0.7431694 , 0.6284153 ,\n",
       "        0.6284153 ,        nan,        nan,        nan,        nan,\n",
       "        0.89071038, 0.89071038, 0.73770492, 0.73770492, 0.63387978,\n",
       "        0.63387978,        nan,        nan,        nan,        nan,\n",
       "        0.89617486, 0.89617486, 0.74863388, 0.74863388, 0.62295082,\n",
       "        0.62295082,        nan,        nan,        nan,        nan,\n",
       "        0.90163934, 0.90163934, 0.72677596, 0.72677596, 0.63387978,\n",
       "        0.63387978,        nan,        nan,        nan,        nan,\n",
       "        0.8852459 , 0.8852459 , 0.73224044, 0.73224044, 0.62295082,\n",
       "        0.62295082,        nan,        nan,        nan,        nan,\n",
       "        0.89071038, 0.89617486, 0.69945355, 0.70491803, 0.65027322,\n",
       "        0.64480874,        nan,        nan,        nan,        nan,\n",
       "        0.92896175, 0.92896175, 0.72677596, 0.72677596, 0.63387978,\n",
       "        0.63387978,        nan,        nan,        nan,        nan]),\n",
       " 'split4_test_score': array([0.89071038, 0.89071038, 0.72131148, 0.72131148, 0.66120219,\n",
       "        0.66120219,        nan,        nan,        nan,        nan,\n",
       "        0.89617486, 0.89617486, 0.74863388, 0.74863388, 0.66666667,\n",
       "        0.66666667,        nan,        nan,        nan,        nan,\n",
       "        0.90163934, 0.90163934, 0.71584699, 0.71584699, 0.66666667,\n",
       "        0.66666667,        nan,        nan,        nan,        nan,\n",
       "        0.90163934, 0.90163934, 0.77595628, 0.77595628, 0.67213115,\n",
       "        0.67213115,        nan,        nan,        nan,        nan,\n",
       "        0.89617486, 0.89617486, 0.72131148, 0.72131148, 0.66666667,\n",
       "        0.66666667,        nan,        nan,        nan,        nan,\n",
       "        0.90163934, 0.90163934, 0.76502732, 0.76502732, 0.67213115,\n",
       "        0.67213115,        nan,        nan,        nan,        nan,\n",
       "        0.86338798, 0.86338798, 0.72131148, 0.72131148, 0.66666667,\n",
       "        0.66666667,        nan,        nan,        nan,        nan,\n",
       "        0.91256831, 0.91256831, 0.7704918 , 0.7704918 , 0.68852459,\n",
       "        0.68852459,        nan,        nan,        nan,        nan,\n",
       "        0.84153005, 0.84153005, 0.72131148, 0.72131148, 0.66666667,\n",
       "        0.66666667,        nan,        nan,        nan,        nan,\n",
       "        0.89617486, 0.89617486, 0.77595628, 0.77595628, 0.68852459,\n",
       "        0.68852459,        nan,        nan,        nan,        nan,\n",
       "        0.84153005, 0.84153005, 0.72131148, 0.72131148, 0.66666667,\n",
       "        0.66666667,        nan,        nan,        nan,        nan,\n",
       "        0.89617486, 0.89617486, 0.77595628, 0.77595628, 0.68852459,\n",
       "        0.68852459,        nan,        nan,        nan,        nan,\n",
       "        0.81967213, 0.79781421, 0.65027322, 0.64480874, 0.59016393,\n",
       "        0.59562842,        nan,        nan,        nan,        nan,\n",
       "        0.80327869, 0.76502732, 0.6010929 , 0.63934426, 0.59016393,\n",
       "        0.6010929 ,        nan,        nan,        nan,        nan,\n",
       "        0.81967213, 0.80874317, 0.62295082, 0.62295082, 0.59016393,\n",
       "        0.58469945,        nan,        nan,        nan,        nan,\n",
       "        0.78688525, 0.80327869, 0.62295082, 0.63387978, 0.59562842,\n",
       "        0.58469945,        nan,        nan,        nan,        nan,\n",
       "        0.81967213, 0.80874317, 0.61748634, 0.6284153 , 0.59016393,\n",
       "        0.59016393,        nan,        nan,        nan,        nan,\n",
       "        0.7704918 , 0.76502732, 0.62295082, 0.60655738, 0.59562842,\n",
       "        0.6010929 ,        nan,        nan,        nan,        nan,\n",
       "        0.78688525, 0.83606557, 0.6010929 , 0.63387978, 0.59562842,\n",
       "        0.59016393,        nan,        nan,        nan,        nan,\n",
       "        0.78688525, 0.77595628, 0.66666667, 0.63387978, 0.59016393,\n",
       "        0.58469945,        nan,        nan,        nan,        nan,\n",
       "        0.90710383, 0.90710383, 0.68306011, 0.68852459, 0.61202186,\n",
       "        0.58469945,        nan,        nan,        nan,        nan,\n",
       "        0.89071038, 0.89071038, 0.73770492, 0.69945355, 0.62295082,\n",
       "        0.61748634,        nan,        nan,        nan,        nan,\n",
       "        0.91803279, 0.89617486, 0.70491803, 0.70491803, 0.59562842,\n",
       "        0.61202186,        nan,        nan,        nan,        nan,\n",
       "        0.90710383, 0.89617486, 0.69945355, 0.69945355, 0.62295082,\n",
       "        0.63387978,        nan,        nan,        nan,        nan,\n",
       "        0.90163934, 0.90710383, 0.71584699, 0.71038251, 0.58469945,\n",
       "        0.59016393,        nan,        nan,        nan,        nan,\n",
       "        0.89617486, 0.92349727, 0.71584699, 0.73770492, 0.6284153 ,\n",
       "        0.64480874,        nan,        nan,        nan,        nan,\n",
       "        0.90163934, 0.90163934, 0.68852459, 0.69945355, 0.6010929 ,\n",
       "        0.6284153 ,        nan,        nan,        nan,        nan,\n",
       "        0.89617486, 0.91256831, 0.69398907, 0.72677596, 0.63934426,\n",
       "        0.63934426,        nan,        nan,        nan,        nan,\n",
       "        0.89617486, 0.90163934, 0.74863388, 0.73224044, 0.68852459,\n",
       "        0.68852459,        nan,        nan,        nan,        nan,\n",
       "        0.90710383, 0.91256831, 0.76502732, 0.75956284, 0.68306011,\n",
       "        0.68306011,        nan,        nan,        nan,        nan,\n",
       "        0.91803279, 0.90163934, 0.71584699, 0.73224044, 0.69945355,\n",
       "        0.70491803,        nan,        nan,        nan,        nan,\n",
       "        0.91256831, 0.90710383, 0.75956284, 0.75956284, 0.68852459,\n",
       "        0.68852459,        nan,        nan,        nan,        nan,\n",
       "        0.91256831, 0.91256831, 0.7431694 , 0.73224044, 0.69945355,\n",
       "        0.71038251,        nan,        nan,        nan,        nan,\n",
       "        0.91256831, 0.90163934, 0.76502732, 0.76502732, 0.68852459,\n",
       "        0.68852459,        nan,        nan,        nan,        nan,\n",
       "        0.89617486, 0.90163934, 0.78142077, 0.79781421, 0.67213115,\n",
       "        0.67213115,        nan,        nan,        nan,        nan,\n",
       "        0.87431694, 0.89617486, 0.74863388, 0.74863388, 0.67213115,\n",
       "        0.66666667,        nan,        nan,        nan,        nan,\n",
       "        0.87431694, 0.84153005, 0.72677596, 0.49726776, 0.49726776,\n",
       "        0.49726776,        nan,        nan,        nan,        nan,\n",
       "        0.50273224, 0.50273224, 0.50273224, 0.49726776, 0.50273224,\n",
       "        0.49726776,        nan,        nan,        nan,        nan,\n",
       "        0.86338798, 0.86338798, 0.72677596, 0.72677596, 0.68852459,\n",
       "        0.68852459,        nan,        nan,        nan,        nan,\n",
       "        0.86338798, 0.86338798, 0.74863388, 0.74863388, 0.68306011,\n",
       "        0.68306011,        nan,        nan,        nan,        nan,\n",
       "        0.86338798, 0.86338798, 0.72677596, 0.72677596, 0.68852459,\n",
       "        0.68852459,        nan,        nan,        nan,        nan,\n",
       "        0.86338798, 0.86338798, 0.74863388, 0.74863388, 0.68306011,\n",
       "        0.68306011,        nan,        nan,        nan,        nan,\n",
       "        0.86338798, 0.86338798, 0.72677596, 0.72677596, 0.68852459,\n",
       "        0.68852459,        nan,        nan,        nan,        nan,\n",
       "        0.86338798, 0.86338798, 0.74863388, 0.74863388, 0.68306011,\n",
       "        0.68306011,        nan,        nan,        nan,        nan,\n",
       "        0.87431694, 0.87431694, 0.72677596, 0.72677596, 0.68852459,\n",
       "        0.68852459,        nan,        nan,        nan,        nan,\n",
       "        0.86885246, 0.86885246, 0.7431694 , 0.7431694 , 0.66666667,\n",
       "        0.66666667,        nan,        nan,        nan,        nan,\n",
       "        0.91256831, 0.91256831, 0.68852459, 0.68852459, 0.65027322,\n",
       "        0.65027322,        nan,        nan,        nan,        nan,\n",
       "        0.87978142, 0.87978142, 0.73224044, 0.73224044, 0.68306011,\n",
       "        0.68306011,        nan,        nan,        nan,        nan,\n",
       "        0.89617486, 0.89617486, 0.69945355, 0.69945355, 0.65027322,\n",
       "        0.65027322,        nan,        nan,        nan,        nan,\n",
       "        0.89617486, 0.89617486, 0.73770492, 0.73770492, 0.63934426,\n",
       "        0.63934426,        nan,        nan,        nan,        nan,\n",
       "        0.89617486, 0.89617486, 0.69398907, 0.69398907, 0.65027322,\n",
       "        0.65027322,        nan,        nan,        nan,        nan,\n",
       "        0.90163934, 0.90163934, 0.73224044, 0.73224044, 0.63934426,\n",
       "        0.63934426,        nan,        nan,        nan,        nan,\n",
       "        0.90710383, 0.90710383, 0.68306011, 0.68306011, 0.65027322,\n",
       "        0.65027322,        nan,        nan,        nan,        nan,\n",
       "        0.8852459 , 0.8852459 , 0.70491803, 0.70491803, 0.64480874,\n",
       "        0.64480874,        nan,        nan,        nan,        nan,\n",
       "        0.86338798, 0.86338798, 0.71584699, 0.71038251, 0.67759563,\n",
       "        0.67759563,        nan,        nan,        nan,        nan,\n",
       "        0.91803279, 0.91803279, 0.7704918 , 0.7704918 , 0.67759563,\n",
       "        0.67759563,        nan,        nan,        nan,        nan]),\n",
       " 'mean_test_score': array([0.88553694, 0.88553694, 0.70017819, 0.70017819, 0.65110478,\n",
       "        0.65110478,        nan,        nan,        nan,        nan,\n",
       "        0.89098361, 0.89098361, 0.71217035, 0.71217035, 0.65870753,\n",
       "        0.65870753,        nan,        nan,        nan,        nan,\n",
       "        0.89970302, 0.89970302, 0.6925398 , 0.6925398 , 0.64779639,\n",
       "        0.64779639,        nan,        nan,        nan,        nan,\n",
       "        0.8942623 , 0.8942623 , 0.72527322, 0.72527322, 0.65980043,\n",
       "        0.65980043,        nan,        nan,        nan,        nan,\n",
       "        0.88660608, 0.88660608, 0.69473153, 0.69473153, 0.64343074,\n",
       "        0.64343074,        nan,        nan,        nan,        nan,\n",
       "        0.90188287, 0.90188287, 0.71654193, 0.71654193, 0.65434783,\n",
       "        0.65434783,        nan,        nan,        nan,        nan,\n",
       "        0.86587669, 0.86587669, 0.69581848, 0.69581848, 0.64343074,\n",
       "        0.64343074,        nan,        nan,        nan,        nan,\n",
       "        0.90842243, 0.90842243, 0.71546686, 0.71546686, 0.65762651,\n",
       "        0.65762651,        nan,        nan,        nan,        nan,\n",
       "        0.85715134, 0.85715134, 0.69581848, 0.69581848, 0.64343074,\n",
       "        0.64343074,        nan,        nan,        nan,        nan,\n",
       "        0.89752316, 0.89752316, 0.71765265, 0.71765265, 0.65762651,\n",
       "        0.65762651,        nan,        nan,        nan,        nan,\n",
       "        0.85715134, 0.85715134, 0.69581848, 0.69581848, 0.64343074,\n",
       "        0.64343074,        nan,        nan,        nan,        nan,\n",
       "        0.89752316, 0.89752316, 0.71765265, 0.71765265, 0.65762651,\n",
       "        0.65762651,        nan,        nan,        nan,        nan,\n",
       "        0.81026372, 0.81243763, 0.63580423, 0.62927061, 0.57036113,\n",
       "        0.56274056,        nan,        nan,        nan,        nan,\n",
       "        0.80484082, 0.80156213, 0.65541102, 0.65871347, 0.57906272,\n",
       "        0.58342837,        nan,        nan,        nan,        nan,\n",
       "        0.80920052, 0.81571632, 0.63908292, 0.62381207, 0.56382751,\n",
       "        0.56382157,        nan,        nan,        nan,        nan,\n",
       "        0.79825968, 0.81682704, 0.65      , 0.6510929 , 0.58124852,\n",
       "        0.58234141,        nan,        nan,        nan,        nan,\n",
       "        0.81790805, 0.81570444, 0.62818365, 0.63253148, 0.56818722,\n",
       "        0.56382157,        nan,        nan,        nan,        nan,\n",
       "        0.80153837, 0.79826562, 0.64891304, 0.64671537, 0.58126039,\n",
       "        0.58234735,        nan,        nan,        nan,        nan,\n",
       "        0.79935852, 0.81464718, 0.61942861, 0.6270967 , 0.56598361,\n",
       "        0.56600143,        nan,        nan,        nan,        nan,\n",
       "        0.80916488, 0.81029342, 0.65983013, 0.65762651, 0.57798171,\n",
       "        0.57362794,        nan,        nan,        nan,        nan,\n",
       "        0.91385127, 0.90620694, 0.69795082, 0.69907341, 0.59326443,\n",
       "        0.59980399,        nan,        nan,        nan,        nan,\n",
       "        0.90187099, 0.90515562, 0.70782846, 0.70019601, 0.60852934,\n",
       "        0.60744833,        nan,        nan,        nan,        nan,\n",
       "        0.9051378 , 0.91057852, 0.69145878, 0.70887384, 0.59435733,\n",
       "        0.59654312,        nan,        nan,        nan,        nan,\n",
       "        0.90734141, 0.91278807, 0.70455571, 0.69691138, 0.60960442,\n",
       "        0.60962224,        nan,        nan,        nan,        nan,\n",
       "        0.9105904 , 0.90624258, 0.69688168, 0.71107745, 0.59217748,\n",
       "        0.59543241,        nan,        nan,        nan,        nan,\n",
       "        0.90405084, 0.91388691, 0.69692326, 0.70784034, 0.60525065,\n",
       "        0.61727251,        nan,        nan,        nan,        nan,\n",
       "        0.9062307 , 0.90187693, 0.6968995 , 0.69904966, 0.5976182 ,\n",
       "        0.60091471,        nan,        nan,        nan,        nan,\n",
       "        0.90950938, 0.91169518, 0.69364457, 0.69584818, 0.61072701,\n",
       "        0.61289499,        nan,        nan,        nan,        nan,\n",
       "        0.90294607, 0.89205868, 0.70784034, 0.70020789, 0.65539914,\n",
       "        0.66085175,        nan,        nan,        nan,        nan,\n",
       "        0.89098361, 0.89533737, 0.71874555, 0.71874555, 0.6630613 ,\n",
       "        0.6630613 ,        nan,        nan,        nan,        nan,\n",
       "        0.90405084, 0.90296389, 0.70564267, 0.70349846, 0.66305536,\n",
       "        0.65433595,        nan,        nan,        nan,        nan,\n",
       "        0.90406272, 0.90078997, 0.72200048, 0.71330482, 0.66416607,\n",
       "        0.66307318,        nan,        nan,        nan,        nan,\n",
       "        0.90842243, 0.90623664, 0.71983844, 0.71219411, 0.66197434,\n",
       "        0.66417201,        nan,        nan,        nan,        nan,\n",
       "        0.90951532, 0.9051378 , 0.71438584, 0.71546092, 0.66306724,\n",
       "        0.6630613 ,        nan,        nan,        nan,        nan,\n",
       "        0.89643027, 0.90298171, 0.71655381, 0.7154728 , 0.66307912,\n",
       "        0.65980043,        nan,        nan,        nan,        nan,\n",
       "        0.88987289, 0.89315158, 0.71325731, 0.71325137, 0.65980043,\n",
       "        0.66198622,        nan,        nan,        nan,        nan,\n",
       "        0.827756  , 0.81792587, 0.54535519, 0.53641601, 0.50162153,\n",
       "        0.51584699,        nan,        nan,        nan,        nan,\n",
       "        0.5027263 , 0.5016334 , 0.5027263 , 0.49944761, 0.5027263 ,\n",
       "        0.4983666 ,        nan,        nan,        nan,        nan,\n",
       "        0.89423854, 0.89423854, 0.71217035, 0.71217035, 0.65870753,\n",
       "        0.65870753,        nan,        nan,        nan,        nan,\n",
       "        0.89861012, 0.89861012, 0.71982062, 0.71982062, 0.65872535,\n",
       "        0.65872535,        nan,        nan,        nan,        nan,\n",
       "        0.89423854, 0.89423854, 0.71217035, 0.71217035, 0.65870753,\n",
       "        0.65870753,        nan,        nan,        nan,        nan,\n",
       "        0.89752316, 0.89752316, 0.71982062, 0.71982062, 0.65981231,\n",
       "        0.65981231,        nan,        nan,        nan,        nan,\n",
       "        0.89423854, 0.89423854, 0.71217035, 0.71217035, 0.65870753,\n",
       "        0.65870753,        nan,        nan,        nan,        nan,\n",
       "        0.89533737, 0.89533737, 0.71872773, 0.71872773, 0.65981231,\n",
       "        0.65981231,        nan,        nan,        nan,        nan,\n",
       "        0.89315752, 0.89315752, 0.70998456, 0.70998456, 0.65870753,\n",
       "        0.65870753,        nan,        nan,        nan,        nan,\n",
       "        0.88770492, 0.88770492, 0.72199454, 0.72199454, 0.65544072,\n",
       "        0.65544072,        nan,        nan,        nan,        nan,\n",
       "        0.89312782, 0.89312782, 0.70231646, 0.70231646, 0.63802566,\n",
       "        0.63802566,        nan,        nan,        nan,        nan,\n",
       "        0.86919102, 0.87245189, 0.67840936, 0.67840936, 0.64240318,\n",
       "        0.64240318,        nan,        nan,        nan,        nan,\n",
       "        0.88766334, 0.88766334, 0.70016631, 0.70016631, 0.63911856,\n",
       "        0.63911856,        nan,        nan,        nan,        nan,\n",
       "        0.88660014, 0.88660014, 0.68061891, 0.68061891, 0.63036945,\n",
       "        0.63036945,        nan,        nan,        nan,        nan,\n",
       "        0.88221668, 0.88221668, 0.70234616, 0.70234616, 0.6380316 ,\n",
       "        0.6380316 ,        nan,        nan,        nan,        nan,\n",
       "        0.88880375, 0.88880375, 0.68500238, 0.68500238, 0.62383583,\n",
       "        0.62383583,        nan,        nan,        nan,        nan,\n",
       "        0.8767819 , 0.8767819 , 0.70124139, 0.70124139, 0.6391245 ,\n",
       "        0.6391245 ,        nan,        nan,        nan,        nan,\n",
       "        0.88225232, 0.87899145, 0.67950226, 0.67950226, 0.62384771,\n",
       "        0.62384771,        nan,        nan,        nan,        nan,\n",
       "        0.86478974, 0.8691435 , 0.6968995 , 0.69362081, 0.66198028,\n",
       "        0.66088738,        nan,        nan,        nan,        nan,\n",
       "        0.91060228, 0.91060228, 0.71328106, 0.71328106, 0.65762057,\n",
       "        0.65762057,        nan,        nan,        nan,        nan]),\n",
       " 'std_test_score': array([0.02823623, 0.02823623, 0.04048286, 0.04048286, 0.03626891,\n",
       "        0.03626891,        nan,        nan,        nan,        nan,\n",
       "        0.01698806, 0.01698806, 0.03926114, 0.03926114, 0.03257634,\n",
       "        0.03257634,        nan,        nan,        nan,        nan,\n",
       "        0.02206009, 0.02206009, 0.03677079, 0.03677079, 0.02083208,\n",
       "        0.02083208,        nan,        nan,        nan,        nan,\n",
       "        0.02432423, 0.02432423, 0.04754322, 0.04754322, 0.02910495,\n",
       "        0.02910495,        nan,        nan,        nan,        nan,\n",
       "        0.0290524 , 0.0290524 , 0.03880069, 0.03880069, 0.01975171,\n",
       "        0.01975171,        nan,        nan,        nan,        nan,\n",
       "        0.01962072, 0.01962072, 0.04797956, 0.04797956, 0.02805241,\n",
       "        0.02805241,        nan,        nan,        nan,        nan,\n",
       "        0.01838351, 0.01838351, 0.03669898, 0.03669898, 0.01975171,\n",
       "        0.01975171,        nan,        nan,        nan,        nan,\n",
       "        0.02383461, 0.02383461, 0.0532124 , 0.0532124 , 0.03076603,\n",
       "        0.03076603,        nan,        nan,        nan,        nan,\n",
       "        0.02443905, 0.02443905, 0.03669898, 0.03669898, 0.01975171,\n",
       "        0.01975171,        nan,        nan,        nan,        nan,\n",
       "        0.02031661, 0.02031661, 0.05451398, 0.05451398, 0.03076603,\n",
       "        0.03076603,        nan,        nan,        nan,        nan,\n",
       "        0.02443905, 0.02443905, 0.03669898, 0.03669898, 0.01975171,\n",
       "        0.01975171,        nan,        nan,        nan,        nan,\n",
       "        0.02031661, 0.02031661, 0.05451398, 0.05451398, 0.03076603,\n",
       "        0.03076603,        nan,        nan,        nan,        nan,\n",
       "        0.02478303, 0.02266362, 0.03940009, 0.03525897, 0.03245535,\n",
       "        0.03437305,        nan,        nan,        nan,        nan,\n",
       "        0.01978453, 0.03056769, 0.04061463, 0.04177039, 0.02985907,\n",
       "        0.03006393,        nan,        nan,        nan,        nan,\n",
       "        0.02424991, 0.0367189 , 0.03354413, 0.0316233 , 0.02829426,\n",
       "        0.02686409,        nan,        nan,        nan,        nan,\n",
       "        0.02029765, 0.02912421, 0.0384248 , 0.03999823, 0.02777523,\n",
       "        0.02643144,        nan,        nan,        nan,        nan,\n",
       "        0.01613184, 0.01764801, 0.03960469, 0.04451174, 0.03103388,\n",
       "        0.03018235,        nan,        nan,        nan,        nan,\n",
       "        0.03007323, 0.02857073, 0.04167001, 0.04095086, 0.02339899,\n",
       "        0.03383164,        nan,        nan,        nan,        nan,\n",
       "        0.02456944, 0.03112817, 0.03773122, 0.03788527, 0.02485176,\n",
       "        0.02496731,        nan,        nan,        nan,        nan,\n",
       "        0.02305506, 0.03792521, 0.03768857, 0.0345863 , 0.02999251,\n",
       "        0.02505522,        nan,        nan,        nan,        nan,\n",
       "        0.00869275, 0.01402259, 0.03647193, 0.04182457, 0.02853385,\n",
       "        0.02533967,        nan,        nan,        nan,        nan,\n",
       "        0.02252249, 0.02209393, 0.0401895 , 0.05111893, 0.02681414,\n",
       "        0.03207289,        nan,        nan,        nan,        nan,\n",
       "        0.01630288, 0.01872887, 0.03964577, 0.03600742, 0.02992976,\n",
       "        0.02766194,        nan,        nan,        nan,        nan,\n",
       "        0.02268488, 0.02086132, 0.0463424 , 0.03830191, 0.02710865,\n",
       "        0.02875697,        nan,        nan,        nan,        nan,\n",
       "        0.02538207, 0.02006283, 0.03082097, 0.03985006, 0.02781501,\n",
       "        0.0292897 ,        nan,        nan,        nan,        nan,\n",
       "        0.02945218, 0.02278748, 0.04770325, 0.04499178, 0.03451136,\n",
       "        0.0301111 ,        nan,        nan,        nan,        nan,\n",
       "        0.0179332 , 0.01964215, 0.03605141, 0.03763919, 0.02335014,\n",
       "        0.03713381,        nan,        nan,        nan,        nan,\n",
       "        0.02626728, 0.02007007, 0.04492   , 0.04411245, 0.0360939 ,\n",
       "        0.03251044,        nan,        nan,        nan,        nan,\n",
       "        0.01107501, 0.01720796, 0.0436635 , 0.04075665, 0.03217176,\n",
       "        0.03106737,        nan,        nan,        nan,        nan,\n",
       "        0.02185475, 0.02200572, 0.04668749, 0.04474931, 0.02855219,\n",
       "        0.02640227,        nan,        nan,        nan,        nan,\n",
       "        0.01554055, 0.01462752, 0.0441977 , 0.04460414, 0.02714063,\n",
       "        0.03005734,        nan,        nan,        nan,        nan,\n",
       "        0.0233622 , 0.02454445, 0.03742373, 0.05626258, 0.03102199,\n",
       "        0.03395266,        nan,        nan,        nan,        nan,\n",
       "        0.01460335, 0.01290224, 0.0520672 , 0.04693387, 0.03217727,\n",
       "        0.03772161,        nan,        nan,        nan,        nan,\n",
       "        0.02336869, 0.02103845, 0.049854  , 0.047357  , 0.03311742,\n",
       "        0.02664695,        nan,        nan,        nan,        nan,\n",
       "        0.02294979, 0.02088088, 0.05140492, 0.05499598, 0.02936479,\n",
       "        0.02511845,        nan,        nan,        nan,        nan,\n",
       "        0.01923361, 0.01796855, 0.03976359, 0.04069006, 0.03291493,\n",
       "        0.03126092,        nan,        nan,        nan,        nan,\n",
       "        0.03427439, 0.0295093 , 0.09079193, 0.04619703, 0.00560237,\n",
       "        0.03174106,        nan,        nan,        nan,        nan,\n",
       "        0.00171864, 0.00277821, 0.00171864, 0.00317511, 0.00171864,\n",
       "        0.00277821,        nan,        nan,        nan,        nan,\n",
       "        0.02915085, 0.02915085, 0.04818701, 0.04818701, 0.03202164,\n",
       "        0.03202164,        nan,        nan,        nan,        nan,\n",
       "        0.03127413, 0.03127413, 0.05683609, 0.05683609, 0.03520377,\n",
       "        0.03520377,        nan,        nan,        nan,        nan,\n",
       "        0.02915085, 0.02915085, 0.04818701, 0.04818701, 0.03202164,\n",
       "        0.03202164,        nan,        nan,        nan,        nan,\n",
       "        0.03103602, 0.03103602, 0.05683609, 0.05683609, 0.033518  ,\n",
       "        0.033518  ,        nan,        nan,        nan,        nan,\n",
       "        0.02915085, 0.02915085, 0.04818701, 0.04818701, 0.03202164,\n",
       "        0.03202164,        nan,        nan,        nan,        nan,\n",
       "        0.02886113, 0.02886113, 0.05578905, 0.05578905, 0.033518  ,\n",
       "        0.033518  ,        nan,        nan,        nan,        nan,\n",
       "        0.02607814, 0.02607814, 0.04721728, 0.04721728, 0.03202164,\n",
       "        0.03202164,        nan,        nan,        nan,        nan,\n",
       "        0.02761287, 0.02761287, 0.06285027, 0.06285027, 0.03021639,\n",
       "        0.03021639,        nan,        nan,        nan,        nan,\n",
       "        0.01905794, 0.01905794, 0.03529438, 0.03529438, 0.04014461,\n",
       "        0.04014461,        nan,        nan,        nan,        nan,\n",
       "        0.02792611, 0.02933062, 0.05951782, 0.05951782, 0.04060759,\n",
       "        0.04060759,        nan,        nan,        nan,        nan,\n",
       "        0.027698  , 0.027698  , 0.03642955, 0.03642955, 0.04460404,\n",
       "        0.04460404,        nan,        nan,        nan,        nan,\n",
       "        0.01198407, 0.01198407, 0.05977164, 0.05977164, 0.03100079,\n",
       "        0.03100079,        nan,        nan,        nan,        nan,\n",
       "        0.02571316, 0.02571316, 0.04140265, 0.04140265, 0.04460375,\n",
       "        0.04460375,        nan,        nan,        nan,        nan,\n",
       "        0.02330972, 0.02330972, 0.06189214, 0.06189214, 0.03527644,\n",
       "        0.03527644,        nan,        nan,        nan,        nan,\n",
       "        0.03523161, 0.03523161, 0.03727477, 0.03727477, 0.04801956,\n",
       "        0.04801956,        nan,        nan,        nan,        nan,\n",
       "        0.02384265, 0.02193357, 0.0473784 , 0.0473784 , 0.03550605,\n",
       "        0.03550605,        nan,        nan,        nan,        nan,\n",
       "        0.01921118, 0.0188488 , 0.03744533, 0.03448942, 0.02770363,\n",
       "        0.02824638,        nan,        nan,        nan,        nan,\n",
       "        0.02209053, 0.02209053, 0.05005342, 0.05005342, 0.02884731,\n",
       "        0.02884731,        nan,        nan,        nan,        nan]),\n",
       " 'rank_test_score': array([ 75,  75, 169, 169, 255, 255, 457, 456, 455, 454,  61,  61, 143,\n",
       "        143, 229, 229, 453, 452, 451, 458,  33,  33, 192, 192, 260, 260,\n",
       "        450, 448, 447, 446,  47,  47, 111, 111, 222, 222, 445, 444, 443,\n",
       "        442,  71,  71, 188, 188, 263, 263, 449, 460, 469, 461,  28,  28,\n",
       "        130, 130, 252, 252, 477, 476, 475, 474,  86,  86, 182, 182, 263,\n",
       "        263, 473, 472, 471, 478,  11,  11, 133, 133, 240, 240, 470, 468,\n",
       "        467, 466,  89,  89, 182, 182, 263, 263, 465, 464, 463, 462,  37,\n",
       "         37, 125, 125, 240, 240, 441, 459, 440, 419,  89,  89, 182, 182,\n",
       "        263, 263, 417, 416, 415, 414,  37,  37, 125, 125, 240, 240, 413,\n",
       "        412, 411, 418, 102, 100, 282, 286, 319, 326, 410, 408, 407, 406,\n",
       "        105, 106, 250, 228, 316, 311, 405, 404, 403, 402, 103,  97, 277,\n",
       "        293, 323, 325, 409, 420, 429, 421, 110,  96, 258, 257, 315, 313,\n",
       "        437, 436, 435, 434,  95,  98, 287, 283, 320, 324, 433, 432, 431,\n",
       "        438, 107, 109, 259, 262, 314, 312, 430, 428, 427, 426, 108,  99,\n",
       "        294, 288, 322, 321, 425, 424, 423, 422, 104, 101, 217, 239, 317,\n",
       "        318, 439, 479, 480, 481,   2,  18, 175, 173, 309, 304, 537, 536,\n",
       "        535, 534,  31,  19, 157, 168, 300, 301, 533, 532, 531, 538,  20,\n",
       "          8, 194, 154, 308, 306, 530, 528, 527, 526,  14,   3, 159, 177,\n",
       "        299, 298, 525, 524, 523, 522,   7,  15, 180, 151, 310, 307, 529,\n",
       "        539, 540, 541,  23,   1, 176, 155, 302, 295, 558, 557, 556, 555,\n",
       "         17,  30, 178, 174, 305, 303, 554, 553, 552, 551,  10,   4, 190,\n",
       "        181, 297, 296, 550, 549, 548, 400,  27,  60, 155, 167, 251, 216,\n",
       "        546, 545, 544, 543,  61,  44, 121, 121, 208, 208, 542, 521, 520,\n",
       "        519,  23,  26, 158, 160, 211, 254, 499, 497, 496, 495,  22,  32,\n",
       "        113, 137, 204, 206, 494, 493, 492, 491,  11,  16, 116, 142, 214,\n",
       "        203, 498, 490, 488, 487,   9,  20, 136, 135, 207, 208, 486, 485,\n",
       "        484, 483,  43,  25, 129, 132, 205, 225, 482, 489, 500, 509,  64,\n",
       "         57, 140, 141, 222, 212, 501, 517, 516, 515,  93,  94, 327, 328,\n",
       "        334, 329, 514, 513, 512, 511, 330, 333, 330, 335, 330, 336, 518,\n",
       "        510, 508, 507,  49,  49, 145, 145, 229, 229, 506, 505, 504, 503,\n",
       "         35,  35, 117, 117, 226, 226, 502, 401, 559, 399,  49,  49, 145,\n",
       "        145, 229, 229, 342, 340, 339, 338,  37,  37, 117, 117, 218, 218,\n",
       "        337, 341, 344, 343,  49,  49, 145, 145, 229, 229, 385, 346, 374,\n",
       "        375,  44,  44, 123, 123, 218, 218, 376, 377, 378, 379,  55,  55,\n",
       "        152, 152, 229, 229, 380, 381, 382, 383,  67,  67, 114, 114, 248,\n",
       "        248, 384, 386, 398, 387,  58,  58, 163, 163, 280, 280, 388, 389,\n",
       "        390, 391,  84,  83, 201, 201, 271, 271, 392, 393, 345, 394,  69,\n",
       "         69, 171, 171, 275, 275, 395, 396, 397, 373,  73,  73, 197, 197,\n",
       "        284, 284, 372, 371, 370, 347,  78,  78, 161, 161, 278, 278, 348,\n",
       "        349, 350, 351,  65,  65, 195, 195, 291, 291, 352, 353, 354, 355,\n",
       "         81,  81, 165, 165, 273, 273, 356, 357, 358, 359,  77,  80, 199,\n",
       "        199, 289, 289, 360, 361, 362, 363,  88,  85, 178, 191, 213, 215,\n",
       "        364, 365, 366, 367,   5,   5, 138, 138, 246, 246, 368, 369, 547,\n",
       "        560], dtype=int32)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing some scores\n",
    "grid_fitted.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GridSearchCVBestEstimator.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pickling the best estimator while we're at it\n",
    "joblib.dump(grid_fitted.best_estimator_, 'GridSearchCVBestEstimator.pkl')\n",
    "joblib.dump(grid_fitted.best_estimator_, 'GridSearchCVBestEstimator.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Random Forest Classifier's accuracy on the training set:1.0\n",
      "The best Random Forest Classifier's accuracy on the testing set:0.9213197969543148\n",
      "CPU times: user 1min 27s, sys: 0 ns, total: 1min 27s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Printing GridSearch results with a cross validation of 5 folds \n",
    "print(f\"The best Random Forest Classifier's accuracy on the training set:{grid_fitted.score(X_train, y_train)}\")\n",
    "print(f\"The best Random Forest Classifier's accuracy on the testing set:{grid_fitted.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Verdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best performing model is the Random Forest Classifier with the optimized parameters. However, when comparign this score to the baseline models, it seems to be a few points lower than what we got otherwise. However, what we must keep in mind is that the GridSearch was only run  on 10% of the data while the other models were run on 30% of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To wrap up, we ran over 9 different models in order to identify the way how articles are written. We ran the following models:\n",
    "\n",
    "- Logistic Regression\n",
    "- Logistic Regression with scaling\n",
    "- Decision Tree Classifier\n",
    "- Random Forest Classifier\n",
    "- K Nearest Neighbors Classifier\n",
    "- Naive Bayes Multinomial Classifier\n",
    "- Neural Network MLP Classifier\n",
    "- Ada Boost Classifier\n",
    "- Support Vector Machines Classifier\n",
    "\n",
    "We could've selected one of the baseline models without having to do the grid search but I wanted to find the best model out of all baseline models and hence this outputted the Random Forest Classifier. However, we can see that AdaBoost Classifier performed very well with an accuracy of almost 90% without any hyperparameter optimization. \n",
    "\n",
    "A note to be made here is that applying dimensionality reduction and scaling our data was not necessary as we're only dealing with one specific column here which is `text` from both dataframes in a combined dataframe altogether. Hence, it does not make sense to reduce the dimensionality and scale our data. \n",
    "\n",
    "Based on our detailed analysis, we obviously will either minimize the options to the Ada Boost Classifier or the Random Forest Classifier. In terms of next steps, this is something we could run a grid search on including only Ada Boost and Random Forest to see which one performs the best.\n",
    "\n",
    "\n",
    "Please also feel free to run the application that I've have created using the [Streamlit](https://www.streamlit.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
